{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef954652",
   "metadata": {},
   "source": [
    "# Ajuste del Pronóstico de Temperatura del WRF 1.5\n",
    "\n",
    "### Nota:\n",
    "Recuerde ajustar los links de las carpetas, directorios y url's que se encuentran a lo largo del código."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e97a9",
   "metadata": {},
   "source": [
    "## Cargar Librerias\n",
    "* Se cargan las librerías esenciales para la manipulación de datos, visualización, cálculos numéricos, manejo de fechas, interacción con la web y operaciones del sistema operativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "e1e91713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # Para buscar archivos en un directorio.\n",
    "import pandas as pd # Para manipulación de datos.\n",
    "import numpy as np # Para cálculos numéricos.\n",
    "from datetime import datetime, date, timedelta # Para fechas y horas.\n",
    "import requests # Para realizar solicitudes HTTP.\n",
    "import urllib.request # Para trabajar con URLs.\n",
    "import io # Para manejar entradas/salidas.\n",
    "import os # Para interactuar con el sistema operativo.\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03d9080",
   "metadata": {},
   "source": [
    "## Configuración de Almacenamiento y Fechas para Datos de Temperatura\n",
    "* Se proporciona las de rutas de carpetas ( `carpeta_modelo`, `carpeta_observado`) que se utilizarán para almacenar los datos del modelo y los datos observados de temperatura, respectivamente. Además, se establecen fechas de inicio (`fecha_inicio`) y fin (`fecha_fin`) para cargar archivos de temperatura en un rango específico de días.\n",
    "\n",
    "* El diccionario `meses_espanol` asocia números de meses con sus nombres en español. Además, se definen enlaces base (`enlace_base_mod` y `enlace_base_obs`) que señalan a los servidores donde se encuentran los datos de temperatura pronosticada y observada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "9d4dd5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta donde se almacenará los datos procesados\n",
    "carpeta = r\"C:/Users/arias/OneDrive/Documentos/UCR/TFG/Ajuste_Datos/Temperatura/Datos\"\n",
    "\n",
    "# Definición de la ruta de la carpeta donde se almacenarán los datos.\n",
    "carpeta_modelo = f\"{carpeta}/Modelo\"\n",
    "carpeta_observado = f\"{carpeta}/Observado\"\n",
    "\n",
    "#Asignar rango de fechas para cargar archivos de temperatura\n",
    "fecha_inicio = datetime.now().date() - timedelta(days=4)\n",
    "fecha_fin = datetime.now().date() - timedelta(days=1)\n",
    "\n",
    "# Diccionario que asocia números de meses con sus nombres en español\n",
    "meses_espanol = {\n",
    "    1: \"enero\", 2: \"febrero\", 3: \"marzo\", 4: \"abril\", 5: \"mayo\", 6: \"junio\", 7: \"julio\",\n",
    "    8: \"agosto\", 9: \"setiembre\", 10: \"octubre\", 11: \"noviembre\", 12: \"diciembre\"\n",
    "}\n",
    "\n",
    "# Enlace base al servidor donde se encuentran los datos de temperatura pronosticada.\n",
    "enlace_base_mod = \"https://wrf1-5.imn.ac.cr/modelo/backup/TESIS/\"\n",
    "\n",
    "# Enlace base al servidor donde se encuentran los datos de temperatura observada.\n",
    "enlace_base_obs = \"http://intra-files.imn.ac.cr/Intranet_graficos/datos5minutos/tablas_registro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "839d3367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2023, 12, 3)"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fecha_inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "e10de97f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2023, 12, 6)"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fecha_fin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0daafc",
   "metadata": {},
   "source": [
    "## Verificar que los archivos de Temperatura (Observado y Modelo) de los 3 días anteriores existan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09639294",
   "metadata": {},
   "source": [
    "### Función de Carga y Procesamiento de Datos de Temperatura\n",
    "\n",
    "La función `cargar_datos` gestiona la carga y procesamiento de archivos relacionados con la temperatura. En primer lugar, utiliza la información de la fecha proporcionada, desglosándola en componentes como año, mes y día, así como el nombre del mes en español. Luego, construye los enlaces completos para los archivos de temperatura observada y pronosticada, utilizando estos componentes y las direcciones base proporcionadas.\n",
    "\n",
    "La función realiza verificaciones para asegurarse de que los archivos de temperatura observada y pronosticada estén disponibles mediante solicitudes HTTP a los respectivos enlaces. Si ambos archivos están presentes, la función decodifica y lee el archivo CSV de temperatura pronosticada, verificando la presencia de valores negativos o cadenas en los datos. Si se encuentran tales valores problemáticos, se emite un mensaje y el archivo no se carga. En caso contrario, se almacenan los datos observados, los datos pronosticados y la fecha correspondiente en listas específicas.\n",
    "\n",
    "En situaciones donde uno o ambos archivos no están disponibles, la función emite un mensaje indicando archivos faltantes y agrega la fecha a una lista de datos faltantes. Finalmente, la función devuelve las listas actualizadas de datos pronosticados, observados y las fechas de los archivos procesados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "1dd3bd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar datos y procesar archivos\n",
    "def cargar_datos(fecha, fecha_anterior, COLUMN_NAMES, enlace_base_obs, enlace_base_mod, meses_espanol):\n",
    "    \n",
    "    # Desglosar la fecha en sus componentes: año, mes, día y nombre del mes en español\n",
    "    año, mes, dia, mes_text = fecha.year, fecha.month, fecha.day, meses_espanol[fecha.month]\n",
    "\n",
    "    # Crear el URL completo del archivo de temperatura observada utilizando los componentes de la fecha\n",
    "    url_observado = f\"{enlace_base_obs}/{año}/{mes_text}/temperaturas_{dia}_{mes}_{año}.txt\"\n",
    "\n",
    "    # Formar el enlace completo para el archivo CSV con la fecha actual y siguiente\n",
    "    url_modelo = f\"{enlace_base_mod}{fecha_anterior.strftime('%Y%m%d')}/{fecha.strftime('%Y%m%d')}.temperatura.csv\"\n",
    "\n",
    "    # Verificar si el archivo de temperatura observada existe antes de descargarlo\n",
    "    response_obs = requests.get(url_observado)\n",
    "    response_mod = requests.get(url_modelo)\n",
    "\n",
    "    if response_obs.status_code == 200 and response_mod.status_code == 200:\n",
    "        # Decodificar y leer los datos del archivo CSV de temperatura pronosticada\n",
    "        mod_data = response_mod.content.decode('utf-8')\n",
    "        df = pd.read_csv(io.StringIO(mod_data), header=None, names=COLUMN_NAMES)\n",
    "\n",
    "        # Verificar la presencia de valores negativos o cadenas en el archivo de temperatura pronosticada\n",
    "        negative_values = (df[COLUMN_NAMES[1:]] < 0).any(axis=0)\n",
    "        str_values = df[COLUMN_NAMES[1:]].applymap(lambda x: isinstance(x, str)).any(axis=0)\n",
    "\n",
    "        # Si hay valores problemáticos, mostrar un mensaje y no cargar el archivo\n",
    "        if (negative_values.any() or str_values.any()):\n",
    "            print(f\"Valores de temperatura pronosticada para {fecha} son negativos o del tipo str. No se carga el archivo.\")\n",
    "            missing_data.append(fecha.strftime(\"%Y%m%d\"))\n",
    "        else:\n",
    "            # Almacenar datos observados, pronosticados y la fecha en listas correspondientes\n",
    "            datos_observados[fecha] = response_obs.text\n",
    "            datos_pronostico.append(df)\n",
    "            file_date.append(fecha.strftime('%Y%m%d'))\n",
    "    else:\n",
    "        # Si falta algún archivo, mostrar un mensaje y agregar la fecha a la lista de datos faltantes\n",
    "        print(f\"Hay archivos faltantes para la fecha {fecha}\")\n",
    "        missing_data.append(fecha.strftime(\"%Y%m%d\"))\n",
    "    \n",
    "    return datos_pronostico, datos_observados, file_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c7084",
   "metadata": {},
   "source": [
    "### Gestión de Datos de Temperatura: Descarga, Procesamiento y Manejo de Fechas\n",
    "\n",
    "Se inicia con la definición de un diccionario `datos_observados` para almacenar el contenido de los archivos de temperatura observada, una lista `datos_pronostico` para almacenar datos de pronóstico, otra lista `file_date` para conservar las fechas de los archivos descargados, y una lista `missing_data` para registrar las fechas de archivos faltantes.\n",
    "\n",
    "Se especifica un conjunto de nombres de columnas `COLUMN_NAMES` para el DataFrame que se creará a partir de los datos de pronóstico. La variable `fecha_anterior` se inicializa con la fecha de inicio, y a continuación, se inicia un bucle que itera sobre el rango de fechas entre `fecha_inicio` y `fecha_fin`.\n",
    "\n",
    "Dentro del bucle, se calcula la fecha del archivo a descargar sumando un día a la fecha anterior. Se construyen las URL completas para los archivos de temperatura observada y pronosticada utilizando los componentes de la fecha. Luego, se realizan solicitudes HTTP para verificar la disponibilidad de ambos archivos.\n",
    "\n",
    "Si algún archivo falta, se muestra un mensaje indicando la ausencia y se agrega la fecha a `missing_data`. También, en este caso, se verifica si es necesario cargar datos de temperatura observada de cuatro días atrás.\n",
    "\n",
    "En caso de que ambos archivos estén disponibles, se decodifica y lee el archivo CSV de temperatura pronosticada. Se verifica la presencia de valores negativos o cadenas en los datos y, si se detectan, se emite un mensaje y se añade la fecha a la lista `missing_data`. Además, si hay menos de dos archivos faltantes, se intenta cargar datos de temperatura observada de cuatro días atrás utilizando la función `cargar_datos`.\n",
    "\n",
    "Finalmente, la fecha anterior se actualiza para la próxima iteración del bucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "d78d6758",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_observados = {} # Diccionario para almacenar el contenido de los archivos de temperatura observada\n",
    "\n",
    "datos_pronostico = [] # Lista para almacenar los datos de pronóstico\n",
    "\n",
    "file_date = [] # Lista para almacenar las fechas de los archivos descargados\n",
    "\n",
    "missing_data = []# Lista para almacenar las fechas de archivos faltantes\n",
    "\n",
    "COLUMN_NAMES = [\"Estacion\", \"Tmax\", \"Tmin\"] # Nombres de columnas para el DataFrame\n",
    "\n",
    "fecha_anterior = fecha_inicio # Inicializar la fecha anterior con la fecha de inicio\n",
    "\n",
    "# Iterar sobre el rango de fechas\n",
    "while fecha_anterior < fecha_fin:\n",
    "\n",
    "    fecha = fecha_anterior + timedelta(days=1) # Calcular la fecha del archivo a descargar sumando un día a la fecha anterior\n",
    "\n",
    "    # Desglosar la fecha en sus componentes: año, mes, día y nombre del mes en español\n",
    "    año, mes, dia, mes_text = fecha.year, fecha.month, fecha.day, meses_espanol[fecha.month]\n",
    "\n",
    "    # Crear el URL completo del archivo de temperatura observada utilizando los componentes de la fecha\n",
    "    url_observado = f\"{enlace_base_obs}/{año}/{mes_text}/temperaturas_{dia}_{mes}_{año}.txt\"\n",
    "\n",
    "    # Formar el enlace completo para el archivo CSV con la fecha actual y siguiente\n",
    "    url_modelo = f\"{enlace_base_mod}{fecha_anterior.strftime('%Y%m%d')}/{fecha.strftime('%Y%m%d')}.temperatura.csv\"\n",
    "\n",
    "    # Verificar si el archivo de temperatura observada y del modelo existe antes de descargarlo\n",
    "    response_obs = requests.get(url_observado)\n",
    "    response_mod = requests.get(url_modelo)\n",
    "\n",
    "    # Verificar si ambos archivos están disponibles\n",
    "    if response_obs.status_code == 200 and response_mod.status_code == 200:\n",
    "        # Decodificar y leer los datos del archivo CSV de temperatura pronosticada\n",
    "        mod_data = response_mod.content.decode('utf-8')\n",
    "        df = pd.read_csv(io.StringIO(mod_data), header=None, names=COLUMN_NAMES)\n",
    "\n",
    "        # Verificar la presencia de valores negativos o cadenas en el archivo de temperatura pronosticada\n",
    "        negative_values = (df[COLUMN_NAMES[1:]] < 0).any(axis=0)\n",
    "        str_values = df[COLUMN_NAMES[1:]].applymap(lambda x: isinstance(x, str)).any(axis=0)\n",
    "\n",
    "        # Si hay valores problemáticos, mostrar un mensaje y no cargar el archivo\n",
    "        if (negative_values.any() or str_values.any()):\n",
    "            print(f\"Valores de temperatura pronosticada para la fecha {fecha} son negativos o del tipo str. No se carga el archivo.\")\n",
    "            missing_data.append(fecha.strftime(\"%Y%m%d\")) # Agregar la fecha del archivo faltante\n",
    "\n",
    "            # Verificar si es necesario cargar datos de temperatura observada de cuatro días atrás\n",
    "            if len(missing_data) < 2:\n",
    "                fecha_sust = fecha_inicio  # Obtener la fecha siguiente\n",
    "                fecha_sust_anterior = fecha_inicio - timedelta(days=1)\n",
    "                datos_pronostico, datos_observados, file_date = cargar_datos(fecha_sust, fecha_sust_anterior, COLUMN_NAMES, enlace_base_obs, enlace_base_mod, meses_espanol)\n",
    "            else:\n",
    "                print(f\"Hay más de dos archivos faltantes\")\n",
    "        else:\n",
    "            # Almacenar datos observados, pronosticados y la fecha en listas correspondientes\n",
    "            datos_observados[fecha] = response_obs.text\n",
    "            datos_pronostico.append(df)\n",
    "            file_date.append(fecha.strftime('%Y%m%d'))\n",
    "    else:\n",
    "        # Si falta algún archivo, mostrar un mensaje y agregar la fecha a la lista de datos faltantes\n",
    "        print(f\"Hay archivos faltantes para la fecha {fecha}\")\n",
    "        missing_data.append(fecha.strftime(\"%Y%m%d\"))\n",
    "\n",
    "        # Verificar si es necesario cargar datos de temperatura observada de cuatro días atrás\n",
    "        if len(missing_data) < 2:\n",
    "            fecha_sust = fecha_inicio  # Obtener la fecha siguiente\n",
    "            fecha_sust_anterior = fecha_inicio - timedelta(days=1)\n",
    "            datos_pronostico, datos_observados, file_date = cargar_datos(fecha_sust, fecha_sust_anterior, COLUMN_NAMES, enlace_base_obs, enlace_base_mod, meses_espanol)\n",
    "        else:\n",
    "            print(f\"Hay más de dos archivos faltantes\")\n",
    "\n",
    "    # Actualizar la fecha anterior para la próxima iteración del ciclo\n",
    "    fecha_anterior = fecha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37bce5a",
   "metadata": {},
   "source": [
    "## Dar formato a datos de Temperatura del WRF 1.5\n",
    "\n",
    "Se crea una lista llamada `data_modelo` destinada a almacenar DataFrames del modelo. La iteración se realiza sobre las fechas y los DataFrames del pronóstico, emparejados mediante la función `zip`.\n",
    "\n",
    "Dentro del bucle, se lleva a cabo una operación de preprocesamiento de datos, donde las columnas 'Tmax' y 'Tmin' se convierten a tipo numérico, gestionando posibles errores.\n",
    "\n",
    "Posteriormente, se guardan los DataFrames resultantes en archivos CSV y TXT. Estos archivos se almacenan en la carpeta `WRF_Crudo` dentro de la ruta especificada por la variable `carpeta_modelo`, utilizando los nombres generados a partir de las fechas.\n",
    "\n",
    "Finalmente, cada DataFrame procesado se agrega a la lista `data_modelo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f491da89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista para almacenar los DataFrames del modelo\n",
    "data_modelo = []\n",
    "\n",
    "# Iterar sobre las fechas y los DataFrames del pronóstico\n",
    "for date, df in zip(file_date, datos_pronostico):\n",
    "    \n",
    "    # Convertir las columnas 'Tmax' y 'Tmin' a tipo numérico, manejando errores\n",
    "    df[[\"Tmax\", \"Tmin\"]] = df[[\"Tmax\", \"Tmin\"]].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Guardar el DataFrame en archivos CSV y TXT con el nombre generado\n",
    "    df.to_csv(os.path.join(carpeta_modelo, 'WRF_Crudo', f\"{date}.temperatura.csv\"), index=False)\n",
    "    df.to_csv(os.path.join(carpeta_modelo, 'WRF_Crudo', f\"{date}.temperatura.txt\"), index=False)\n",
    "    \n",
    "    # Agregar el DataFrame a la lista de DataFrames del modelo\n",
    "    data_modelo.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a20a6c",
   "metadata": {},
   "source": [
    "## Procesamiento y Almacenamiento de Datos Observados\n",
    "Se comienza estableciendo una lista vacía denominada `data_observada`, destinada a almacenar los DataFrames resultantes de los datos observados. Posteriormente, se implementa un bucle que itera a través de las parejas de `file_date` (fechas en formato de texto) y el contenido de archivos previamente almacenados en `datos_observados`.\n",
    "\n",
    "Dentro de este bucle, el contenido de cada archivo se divide en líneas utilizando `\"\\n\"` como separador. Se inicia una lista llamada `estaciones` para almacenar la información de las estaciones meteorológicas que será extraída del archivo. Además, se establece una variable llamada `seccion_encontrada` para rastrear si se ha encontrado la sección relevante en el archivo.\n",
    "\n",
    "A continuación, el código procede a procesar cada línea del archivo. Si una línea contiene `\"*****\"` o `\".....\"`, indica que se ha encontrado la sección deseada. Si `seccion_encontrada` es `True` y la línea contiene un guion `\"-\"`, esto indica que contiene datos relevantes para una estación. Se dividen los datos utilizando espacios como separadores y se realiza una limpieza de datos, eliminando espacios en blanco y elementos vacíos. Si los datos extraídos no están vacíos y no comienzan con `\"-----\"`, se extraen la estación, la temperatura máxima y mínima.\n",
    "\n",
    "A continuación, se verifica si los valores de tmax y tmin no son \"NA\" y, en caso de ser válidos, se convierten en tipo `float64`. El valor de la estación se convierte en tipo entero. Los datos de estación válidos se agregan a la lista `estaciones` como tuplas (estacion, tmax, tmin).\n",
    "\n",
    "Una vez procesadas todas las líneas del archivo, la lista `estaciones` se convierte en un DataFrame de Pandas llamado `df_estaciones` con columnas \"Estacion\", \"Tmax\" y \"Tmin\". Posteriormente, se genera una ruta de carpeta `carpeta` donde se guardarán los archivos resultantes.\n",
    "\n",
    "Finalmente, el DataFrame `df_estaciones` se agrega a la lista `data_observada`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "89f02619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista para almacenar los DataFrames de datos de temperatura observada luego de ajustar el formato\n",
    "data_observada = []\n",
    "\n",
    "# Iterar sobre las fechas y DataFrames de temperatura observada\n",
    "for date, archivo in zip(file_date, datos_observados.values()):\n",
    "    # Dividir el contenido del archivo en líneas usando \"\\n\" como separador\n",
    "    lineas = archivo.strip().split(\"\\n\")[1:]  # Omitir la primera línea que es la cabecera\n",
    "\n",
    "    # Inicializar una lista para almacenar los datos de las estaciones de este archivo\n",
    "    estaciones = []\n",
    "\n",
    "    # Variable para determinar si hemos encontrado la sección con estaciones\n",
    "    seccion_encontrada = False\n",
    "\n",
    "    # Procesar cada línea para extraer los datos de las estaciones\n",
    "    for linea in lineas:\n",
    "        if \"*****\" in linea:\n",
    "            seccion_encontrada = True\n",
    "        elif \".....\" in linea:\n",
    "            seccion_encontrada = True\n",
    "        elif seccion_encontrada and \"-\" in linea:\n",
    "            datos_estacion = linea.split(\"   \")\n",
    "\n",
    "            # Eliminar líneas vacías antes de procesar los datos\n",
    "            datos_estacion = [dato.strip() for dato in datos_estacion if dato.strip()]\n",
    "\n",
    "            # Asegurarnos de que la línea no esté vacía después de la eliminación\n",
    "            if datos_estacion and not datos_estacion[0].startswith(\"-----\"):\n",
    "                estacion = datos_estacion[0].split(\"-\")[0]\n",
    "                tmax = datos_estacion[4]  # Corregir el índice para obtener la longitud\n",
    "                tmin = datos_estacion[5]   # Corregir el índice para obtener la latitud\n",
    "                \n",
    "                # Solo usar las lineas donde tmax, tmin es diferente a 'NA'\n",
    "                if tmax != \"NA\" and tmin != \"NA\":\n",
    "                    tmax = float(tmax)# Convertir el valor a tipo float64\n",
    "                    tmin = float(tmin)# Convertir el valor a tipo float64\n",
    "                    estacion = int(estacion)# Convertir el valor a tipo int\n",
    "                    \n",
    "                    # Agregar los datos de la estación, tmax y tmin a la lista estaciones\n",
    "                    estaciones.append((estacion, tmax, tmin))\n",
    "\n",
    "    # Convertir la lista de estaciones en un DataFrame de Pandas para este archivo\n",
    "    df_estaciones = pd.DataFrame(estaciones, columns=[\"Estacion\", \"Tmax\", \"Tmin\"])\n",
    "\n",
    "    # Guardar el DataFrame en un archivo CSV con el nombre generado\n",
    "    df_estaciones.to_csv(os.path.join(carpeta_observado, 'Crudo', f\"{date}.temperatura.csv\"), index=False)\n",
    "    df_estaciones.to_csv(os.path.join(carpeta_observado, 'Crudo', f\"{date}.temperatura.txt\"), index=False)\n",
    "\n",
    "    # Agregar el DataFrame con el nuevo formato a la lista de DataFrames\n",
    "    data_observada.append(df_estaciones)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10d395f",
   "metadata": {},
   "source": [
    "## Crear Función para Cargar las Tablas de la Climatología Mensual\n",
    "\n",
    "La función `cargar_clim` está diseñada para facilitar la carga de tablas de climatología mensual de la temperatura desde un archivo Excel. El usuario debe proporcionar la ruta del archivo Excel y una lista de nombres de hojas que desea cargar. La función utiliza la biblioteca Pandas para leer cada hoja especificada, convierte los nombres de las columnas a cadenas de texto y organiza los resultados en un diccionario. Este diccionario tiene como claves los nombres de las hojas y como valores los DataFrames correspondientes, permitiendo un acceso sencillo y organizado a los datos climatológicos cargados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "180249e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_clim(ruta_excel, hojas_excel):\n",
    "    \"\"\"\n",
    "    Cargar las tablas de la climatología mensual de la temperatura observada\n",
    "    \n",
    "    Parameters:\n",
    "        ruta_excel (str): Ruta del archivo Excel que contiene las tablas.\n",
    "        hojas_excel (list): Lista de nombres de hojas de Excel a cargar.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario que contiene las tablas cargadas. Las claves son los nombres de las hojas y los valores son DataFrames de Pandas.\n",
    "    \"\"\"\n",
    "    # Crear un diccionario para almacenar las tablas de la climatología mensual (temperatura observada) cargadas desde Excel\n",
    "    clim = {}\n",
    "    \n",
    "    # Iterar sobre la lista de nombres de hojas de Excel a cargar\n",
    "    for hoja in hojas_excel:\n",
    "        # Cargar la tabla desde la hoja de Excel especificada\n",
    "        tabla = pd.read_excel(ruta_excel, sheet_name=hoja)\n",
    "        \n",
    "        # Convertir los nombres de las columnas a cadenas de texto\n",
    "        tabla.columns = tabla.columns.astype(str)\n",
    "        \n",
    "        # Agregar la tabla al diccionario utilizando el nombre de la hoja como clave\n",
    "        clim[hoja] = tabla\n",
    "\n",
    "    return clim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e5486",
   "metadata": {},
   "source": [
    "### Carga de Tablas Climatología Mensual Observada\n",
    "\n",
    "En el código proporcionado, se define la variable `ruta_observado` como la ruta completa de un archivo Excel que contiene la climatología mensual de la temperatura observada. Además, se especifica una lista de nombres de hojas relevantes dentro de ese archivo. Luego, se utiliza la función `cargar_clim` con los parámetros `ruta_observado` y `hojas_observado` para cargar las tablas correspondientes desde el archivo Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "4cfeac6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo Excel que contiene la climatología mensual de temperatura observada.\n",
    "ruta_observado = 'C:/Users/arias/OneDrive/Documentos/UCR/TFG/Climatologia/Observado/Temperatura/Climatologia_Mensual_Observada_Temperatura.xlsx'\n",
    "hojas_observado = ['tmax_mean', 'tmin_mean', 'tmax_max', 'tmin_max', 'tmax_min', 'tmin_min']\n",
    "\n",
    "#Cargar las tablas de la climatología mensual de la temperatura observada\n",
    "clim_observado = cargar_clim(ruta_observado, hojas_observado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51453fcf",
   "metadata": {},
   "source": [
    "### Carga de Tablas Climatología Mensual del Modelo\n",
    "\n",
    "En el código proporcionado, se define la variable `ruta_modelo` como la ruta completa de un archivo Excel que contiene la climatología mensual de la temperatura pronosticada. Además, se especifica una lista de nombres de hojas relevantes dentro de ese archivo. Luego, se utiliza la función `cargar_clim` con los parámetros `ruta_modelo` y `hojas_modelo` para cargar las tablas correspondientes desde el archivo Excel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "f8695868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta del archivo Excel que contiene la climatología mensual de temperatura observada.\n",
    "ruta_modelo = 'C:/Users/arias/OneDrive/Documentos/UCR/TFG/Climatologia/Modelo/Temperatura/Climatologia_Mensual_Modelo_Temperatura.xlsx'\n",
    "hojas_modelo = ['tmax_mean', 'tmin_mean']\n",
    "\n",
    "#Cargar las tablas de la climatología mensual de la temperatura observada\n",
    "clim_modelo = cargar_clim(ruta_modelo, hojas_modelo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac9c100",
   "metadata": {},
   "source": [
    "## Control de Calidad de Datos con Climatología Mensual Observada\n",
    "\n",
    "### Datos de Pronóstico\n",
    "\n",
    "El código comienza verificando si la variable `data_modelo` no es `None`, indicando la presencia de datos. Si `data_modelo` es `None`, se imprime un mensaje indicando que no se pudieron obtener los datos adecuadamente. Luego, se emplea un bucle `for` para iterar sobre las fechas (`date`) en `file_date` y los datos (`file`) en `data_modelo`.\n",
    "\n",
    "En cada iteración, se convierte la fecha en formato de cadena (`date`) a un objeto `datetime`. Además, se construyen las rutas completas para dos archivos CSV (`tabla_cc_tmax` y `tabla_cc_tmin`) que contendrán tablas de control de calidad. Estas rutas incluyen la carpeta especificada y el año de la fecha actual. Se verifica la existencia de los archivos `ruta_tmax_cc` y `ruta_tmin_cc` en el sistema de archivos. Si existen, se cargan los datos en dos DataFrames; de lo contrario, se crean los DataFrames.\n",
    "\n",
    "Luego, se verifica si la fecha actual ya existe en las tablas `tabla_cc_tmax` o `tabla_cc_tmin`. En caso afirmativo, se eliminan las filas correspondientes. Se crean nuevas filas en los DataFrames `tabla_cc_tmax` y `tabla_cc_tmin` con valores iniciales vacíos y la fecha actual.\n",
    "\n",
    "A continuación, se inicia un bucle para iterar sobre las estaciones presentes en `tabla_cc_tmax` (`tabla_cc_tmin`). Para cada estación y mes correspondiente, se obtienen los límites climatológicos mensuales de temperatura máxima (`tmax_min` y `tmax_max`) desde el DataFrame `clim_observado`. Se realiza una operación similar para obtener los límites climatológicos de temperatura mínima (`tmin_min` y `tmin_max`).\n",
    "\n",
    "Se filtran las filas del DataFrame `file` que corresponden a la estación actual. Se verifica si hay datos disponibles para la estación en el DataFrame `file`. Si hay datos, se obtienen los valores pronosticados de temperatura máxima (`Tmax`) y mínima (`Tmin`) para la estación actual desde el DataFrame `file`.\n",
    "\n",
    "Las temperaturas pronosticadas (`Tmax` y `Tmin`) se ajustan según los límites climatológicos. Si el valor pronosticado excede el límite máximo, se ajusta al valor máximo; si es menor que el límite mínimo, se ajusta al valor mínimo. Si está dentro de los límites, se asigna un guion (`-`). Si no hay datos para la estación en el DataFrame `file`, se asigna el valor \"x\" a las columnas correspondientes en `tabla_tmax_fila` y `tabla_tmin_fila`. \n",
    "Posteriormente, los diccionarios `tabla_tmax_fila` y `tabla_tmin_fila` se convierten en DataFrames.\n",
    "\n",
    "Finalmente, las tablas de control de calidad actualizadas se guardan en archivos CSV y TXT correspondientes a las temperaturas máximas y mínimas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a89fca8",
   "metadata": {},
   "source": [
    "### Datos de Pronostico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "224e2d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si data_modelo es None\n",
    "if data_modelo is not None:\n",
    "    for date, file in zip(file_date, data_modelo):\n",
    "\n",
    "        fecha = datetime.strptime(date, \"%Y%m%d\")\n",
    "        \n",
    "        # Ruta completa del archivo que contendrá la tabla de control de calidad\n",
    "        ruta_tmax_cc = os.path.join(carpeta, f\"tabla_cc_tmax_{fecha.year}.csv\")\n",
    "        ruta_tmin_cc = os.path.join(carpeta, f\"tabla_cc_tmin_{fecha.year}.csv\")\n",
    "\n",
    "        # Si el archivo no existe, se crea la tabla vacía y se descargan y procesan los datos\n",
    "        if os.path.exists(ruta_tmax_cc) and os.path.exists(ruta_tmin_cc):\n",
    "            # Continuar con el procesamiento\n",
    "            tabla_cc_tmax = pd.read_csv(ruta_tmax_cc)\n",
    "            tabla_cc_tmin = pd.read_csv(ruta_tmin_cc)\n",
    "        else:\n",
    "            # Si el archivo existe, cargar la tabla y procesar el día siguiente\n",
    "            tabla_cc_tmax = pd.DataFrame(columns=['FECHA'] + [str(col) for col in data_modelo[0]['Estacion']])\n",
    "            tabla_cc_tmin = pd.DataFrame(columns=['FECHA'] + [str(col) for col in data_modelo[0]['Estacion']])\n",
    "\n",
    "        # Convertir fecha_siguiente_str a np.int64 para hacer la comparación correctamente\n",
    "        fecha_siguiente_int = np.int64(date)\n",
    "\n",
    "        # Verificar si la fecha ya existe en la tabla\n",
    "        if (tabla_cc_tmax[\"FECHA\"] == fecha_siguiente_int).any() or (tabla_cc_tmin[\"FECHA\"] == fecha_siguiente_int).any():\n",
    "            # Si la fecha ya existe, obtener el índice de la fila existente\n",
    "            idx_fecha_existente_tmax = tabla_cc_tmax.loc[tabla_cc_tmax[\"FECHA\"] == fecha_siguiente_int].index[0]\n",
    "            idx_fecha_existente_tmin = tabla_cc_tmin.loc[tabla_cc_tmin[\"FECHA\"] == fecha_siguiente_int].index[0]\n",
    "\n",
    "            # Eliminar la fila existente a partir del índice\n",
    "            if idx_fecha_existente_tmax is not None:\n",
    "                tabla_cc_tmax = tabla_cc_tmax.drop(idx_fecha_existente_tmax)\n",
    "\n",
    "            if idx_fecha_existente_tmin is not None:\n",
    "                tabla_cc_tmin = tabla_cc_tmin.drop(idx_fecha_existente_tmin)\n",
    "\n",
    "        # Crear una nueva fila con la fecha actual\n",
    "        tabla_tmax_fila = {col: \"\" for col in tabla_cc_tmax.columns}\n",
    "        tabla_tmax_fila[\"FECHA\"] = date\n",
    "        \n",
    "        tabla_tmin_fila = {col: \"\" for col in tabla_cc_tmin.columns}\n",
    "        tabla_tmin_fila[\"FECHA\"] = date\n",
    "\n",
    "        columnas_temperatura = ['tmax_min', 'tmax_max', 'tmin_min', 'tmin_max']\n",
    "\n",
    "        for estacion in tabla_cc_tmax.columns[1:]:\n",
    "            if any(estacion in clim_observado[col].columns for col in columnas_temperatura):\n",
    "\n",
    "                # Obtener el valor de climatología mensual de temperatura observada (Tmax) para la estación y el mes correspondiente\n",
    "                valor_tmax_min = clim_observado['tmax_min'].loc[clim_observado['tmax_min'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "                valor_tmax_max = clim_observado['tmax_max'].loc[clim_observado['tmax_max'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "\n",
    "                # Obtener el valor de climatología mensual de temperatura observada (Tmin) para la estación y el mes correspondiente\n",
    "                valor_tmin_min = clim_observado['tmin_min'].loc[clim_observado['tmin_min'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "                valor_tmin_max = clim_observado['tmin_max'].loc[clim_observado['tmin_max'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "\n",
    "                # Verificar si existen filas que coincidan con la estación en file\n",
    "                filas_estacion = file.loc[file[\"Estacion\"].astype(str).str.strip() == estacion.strip()]\n",
    "\n",
    "                # Verificar que la fila para la 'estación' en temperatura pronosticada no esté vacía\n",
    "                if not filas_estacion.empty:\n",
    "                    # Obtener el valor de temperatura pronosticada (Tmax, Tmin) para la estación y el mes correspondiente\n",
    "                    valor_tmax_modelo = filas_estacion[\"Tmax\"].iloc[0]\n",
    "                    valor_tmin_modelo = filas_estacion[\"Tmin\"].iloc[0]\n",
    "\n",
    "                    # Ajustar la temperatura pronosticada (tmax) con los limites de la climatología mensual de temperatura observada (tmax)\n",
    "                    if valor_tmax_modelo >= valor_tmax_max:\n",
    "                        tabla_tmax_fila[estacion] = f\"{valor_tmax_max}({valor_tmax_modelo}>Max)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmax\"] = valor_tmax_max\n",
    "                    elif valor_tmax_modelo < valor_tmax_min:\n",
    "                        tabla_tmax_fila[estacion] = f\"{valor_tmax_min}({valor_tmax_modelo}<Min)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmax\"] = valor_tmax_min\n",
    "                    else:\n",
    "                        tabla_tmax_fila[estacion] = \"-\" # Si el valor está dentro de los limites, asignar \"-\"\n",
    "\n",
    "                    # Ajustar la temperatura pronosticada (tmin) con los limites de la climatología mensual de temperatura observada (tmin)\n",
    "                    if valor_tmin_modelo >= valor_tmin_max:\n",
    "                        tabla_tmin_fila[estacion] = f\"{valor_tmin_max}({valor_tmin_modelo}>Max)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmin\"] = valor_tmin_max\n",
    "                    elif valor_tmin_modelo < valor_tmin_min:\n",
    "                        tabla_tmin_fila[estacion] = f\"{valor_tmin_min}({valor_tmin_modelo}<Min)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmin\"] = valor_tmin_min\n",
    "                    else:\n",
    "                        tabla_tmin_fila[estacion] = \"-\" # Si el valor está dentro de los limites, asignar \"-\"\n",
    "                else:\n",
    "                    # Si no hay valor para la estación en data_modelo, asignar \"x\"\n",
    "                    tabla_tmax_fila[estacion] = \"x\"\n",
    "                    tabla_tmin_fila[estacion] = \"x\"\n",
    "                    print(f'La estación {estacion} no existe en los datos de pronostico de temperatura del WRF1.5')\n",
    "            else:\n",
    "                nueva_fila[estacion] = \"x\"\n",
    "                print(f'La estación {estacion} no existe en las columnas de la Climatología de Temperatura Mensual Observada')\n",
    "\n",
    "        # Convertir los diccionarios en Pandas DataFrames\n",
    "        tabla_tmax_fila = pd.DataFrame.from_dict(tabla_tmax_fila, orient='index').T\n",
    "        tabla_tmin_fila = pd.DataFrame.from_dict(tabla_tmin_fila, orient='index').T\n",
    "\n",
    "        # Concatenar los DataFrames\n",
    "        tabla_cc_tmax = pd.concat([tabla_cc_tmax, tabla_tmax_fila], ignore_index=True)\n",
    "        tabla_cc_tmin = pd.concat([tabla_cc_tmin, tabla_tmin_fila], ignore_index=True)\n",
    "\n",
    "        # Guardar la clim_observado de cc y data_modelo en un archivo CSV y TXT\n",
    "        tabla_cc_tmax.to_csv(f\"{carpeta}/tabla_cc_tmax_{fecha.year}.csv\", index=False)\n",
    "        tabla_cc_tmax.to_csv(os.path.join(carpeta, f\"tabla_cc_tmax_{fecha.year}.txt\"), index=False)\n",
    "\n",
    "        tabla_cc_tmin.to_csv(f\"{carpeta}/tabla_cc_tmin_{fecha.year}.csv\", index=False)\n",
    "        tabla_cc_tmin.to_csv(os.path.join(carpeta, f\"tabla_cc_tmin_{fecha.year}.txt\"), index=False)\n",
    "else:\n",
    "        print(\"No se pudieron obtener los datos correctamente ya que el archivo que contiene los datos de pronostico (data_modelo) está vacio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56172e5",
   "metadata": {},
   "source": [
    "### Datos Observados\n",
    "Primero se comprueba si la variable `data_observada` no es `None`. Si tiene datos, procede con el procesamiento. En caso contrario, imprime un mensaje indicando que no se pudieron obtener los datos correctamente.\n",
    "\n",
    "Para cada fecha en `file_date` y su respectivo archivo en `data_observada_cc`: Convierte la fecha a un objeto `datetime`, genera la ruta completa del archivo que contendrá la tabla de control de calidad (`ruta_archivo_tmax_cc` y `ruta_archivo_tmin_cc`), verifica si el archivo ya existe. Si no existe, crea una tabla vacía. Si existe, carga la tabla existente. Convierte la fecha a un número entero para facilitar la comparación con fechas en la tabla.\n",
    "\n",
    "Se verifica si la fecha ya existe en la tabla de Control de Calidad. Si existe, elimina la fila existente. Srea una nueva fila con la fecha y procesa los datos de temperatura pronosticada y observada para cada estación.\n",
    "\n",
    "Par}a el control de calidad de datos de temperatura observada, se compara los valores de temperatura observada con los límites de la climatología mensual de temperatura observada y se actualiza los valores en el archivo original (`file`), marcando aquellos que no cumplen con los límites como `NaN`.\n",
    "\n",
    "Guarda la tabla ajustada en un archivo CSV y TXT. Guarda el archivo original con los datos actualizados en una carpeta específica, utilizando el nombre de la fecha como parte del nombre del archivo.\n",
    "\n",
    "Imprime mensajes de advertencia si una estación no existe en los datos observados de temperatura o en las columnas de la climatología mensual observada. Realiza el mismo procedimiento para Temperatura máxima y temperatura mínima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "ee8b3992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La estación 69695 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84221 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 69753 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 74059 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 72193 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 76057 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 90013 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84255 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 73175 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84293 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 69695 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84221 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 69753 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 74059 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 72193 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 76057 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 90013 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84255 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 73175 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84293 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 69695 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84221 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 69753 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 74059 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 72193 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 76057 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 90013 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84255 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 73175 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n",
      "La estación 84293 no existe en las columnas de la Climatología de Temperatura Mensual Observada\n"
     ]
    }
   ],
   "source": [
    "# Verificar si data_modelo es None\n",
    "if data_observada is not None:\n",
    "    data_observada_cc = copy.deepcopy(data_observada)\n",
    "    for date, file in zip(file_date, data_observada_cc):\n",
    "\n",
    "        fecha = datetime.strptime(date, \"%Y%m%d\")\n",
    "        \n",
    "        # Ruta completa del archivo que contendrá la tabla de control de calidad\n",
    "        ruta_tmax_cc = os.path.join(carpeta, f\"tabla_cc_tmax_observado_{fecha.year}.csv\")\n",
    "        ruta_tmin_cc = os.path.join(carpeta, f\"tabla_cc_tmin_observado_{fecha.year}.csv\")\n",
    "\n",
    "        # Si el archivo no existe, se crea la tabla vacía y se descargan y procesan los datos\n",
    "        if os.path.exists(ruta_tmax_cc) and os.path.exists(ruta_tmin_cc):\n",
    "            # Continuar con el procesamiento\n",
    "            tabla_cc_tmax_observado = pd.read_csv(ruta_tmax_cc)\n",
    "            tabla_cc_tmin_observado = pd.read_csv(ruta_tmin_cc)\n",
    "        else:\n",
    "            # Si el archivo existe, cargar la tabla y procesar el día siguiente\n",
    "            tabla_cc_tmax_observado = pd.DataFrame(columns=['FECHA'] + [str(col) for col in data_observada_cc[0]['Estacion']])\n",
    "            tabla_cc_tmin_observado = pd.DataFrame(columns=['FECHA'] + [str(col) for col in data_observada_cc[0]['Estacion']])\n",
    "\n",
    "        # Convertir fecha_siguiente_str a np.int64 para hacer la comparación correctamente\n",
    "        fecha_siguiente_int = np.int64(date)\n",
    "\n",
    "        # Verificar si la fecha ya existe en la tabla\n",
    "        if (tabla_cc_tmax_observado[\"FECHA\"] == fecha_siguiente_int).any() or (tabla_cc_tmin_observado[\"FECHA\"] == fecha_siguiente_int).any():\n",
    "            # Si la fecha ya existe, obtener el índice de la fila existente\n",
    "            idx_fecha_existente_tmax = tabla_cc_tmax_observado.loc[tabla_cc_tmax_observado[\"FECHA\"] == fecha_siguiente_int].index[0]\n",
    "            idx_fecha_existente_tmin = tabla_cc_tmin_observado.loc[tabla_cc_tmin_observado[\"FECHA\"] == fecha_siguiente_int].index[0]\n",
    "\n",
    "            # Eliminar la fila existente a partir del índice\n",
    "            if idx_fecha_existente_tmax is not None:\n",
    "                tabla_cc_tmax_observado = tabla_cc_tmax_observado.drop(idx_fecha_existente_tmax)\n",
    "\n",
    "            if idx_fecha_existente_tmin is not None:\n",
    "                tabla_cc_tmin_observado = tabla_cc_tmin_observado.drop(idx_fecha_existente_tmin)\n",
    "\n",
    "        # Crear una nueva fila con la fecha actual\n",
    "        tabla_tmax_fila = {col: \"\" for col in tabla_cc_tmax_observado.columns}\n",
    "        tabla_tmax_fila[\"FECHA\"] = date\n",
    "        \n",
    "        tabla_tmin_fila = {col: \"\" for col in tabla_cc_tmin_observado.columns}\n",
    "        tabla_tmin_fila[\"FECHA\"] = date\n",
    "\n",
    "        olumnas_temperatura = ['tmax_min', 'tmax_max', 'tmin_min', 'tmin_max']\n",
    "\n",
    "        for estacion in tabla_cc_tmax_observado.columns[1:]:\n",
    "            if any(estacion in clim_observado[col].columns for col in columnas_temperatura):\n",
    "                # Obtener el valor de climatología mensual de temperatura observada (Tmax) para la estación y el mes correspondiente\n",
    "                valor_tmax_min = clim_observado['tmax_min'].loc[clim_observado['tmax_min'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "                valor_tmax_max = clim_observado['tmax_max'].loc[clim_observado['tmax_max'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "\n",
    "                # Obtener el valor de climatología mensual de temperatura observada (Tmin) para la estación y el mes correspondiente\n",
    "                valor_tmin_min = clim_observado['tmin_min'].loc[clim_observado['tmin_min'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "                valor_tmin_max = clim_observado['tmin_max'].loc[clim_observado['tmin_max'][\"mes\"] == fecha.month, estacion].values[0]\n",
    "\n",
    "                # Verificar si existen filas que coincidan con la estación en file\n",
    "                filas_estacion = file.loc[file[\"Estacion\"].astype(str).str.strip() == estacion.strip()]\n",
    "\n",
    "                # Verificar que la fila para la 'estación' en temperatura pronosticada no esté vacía\n",
    "                if not filas_estacion.empty:\n",
    "                    # Obtener el valor de temperatura pronosticada (Tmax, Tmin) para la estación y el mes correspondiente\n",
    "                    valor_tmax_modelo = filas_estacion[\"Tmax\"].iloc[0]\n",
    "                    valor_tmin_modelo = filas_estacion[\"Tmin\"].iloc[0]\n",
    "\n",
    "                    # Ajustar la temperatura pronosticada (tmax) con los limites de la climatología mensual de temperatura observada (tmax)\n",
    "                    if valor_tmax_modelo >= valor_tmax_max:\n",
    "                        tabla_tmax_fila[estacion] = f\"{valor_tmax_max}({valor_tmax_modelo}>Max)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmax\"] = np.nan\n",
    "                    elif valor_tmax_modelo < valor_tmax_min:\n",
    "                        tabla_tmax_fila[estacion] = f\"{valor_tmax_min}({valor_tmax_modelo}<Min)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmax\"] = np.nan\n",
    "                    else:\n",
    "                        tabla_tmax_fila[estacion] = \"-\" # Si el valor está dentro de los limites, asignar \"-\"\n",
    "\n",
    "                    # Ajustar la temperatura pronosticada (tmin) con los limites de la climatología mensual de temperatura observada (tmin)\n",
    "                    if valor_tmin_modelo >= valor_tmin_max:\n",
    "                        tabla_tmin_fila[estacion] = f\"{valor_tmin_max}({valor_tmin_modelo}>Max)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmin\"] = np.nan\n",
    "                    elif valor_tmin_modelo < valor_tmin_min:\n",
    "                        tabla_tmin_fila[estacion] = f\"{valor_tmin_min}({valor_tmin_modelo}<Min)\"\n",
    "                        file.loc[filas_estacion.index, \"Tmin\"] = np.nan\n",
    "                    else:\n",
    "                        tabla_tmin_fila[estacion] = \"-\" # Si el valor está dentro de los limites, asignar \"-\"\n",
    "                else:\n",
    "                    # Si no hay valor para la estación en data_modelo, asignar \"x\"\n",
    "                    tabla_tmax_fila[estacion] = \"x\"\n",
    "                    tabla_tmin_fila[estacion] = \"x\"\n",
    "                    print(f'La estación {estacion} no existe en los datos de observados de temperatura en la fecha {date}')\n",
    "            else:\n",
    "                tabla_tmax_fila[estacion] = \"x\"\n",
    "                tabla_tmin_fila[estacion] = \"x\"\n",
    "                print(f'La estación {estacion} no existe en las columnas de la Climatología de Temperatura Mensual Observada')\n",
    "\n",
    "        # Convertir los diccionarios en Pandas DataFrames\n",
    "        tabla_tmax_fila = pd.DataFrame.from_dict(tabla_tmax_fila, orient='index').T\n",
    "        tabla_tmin_fila = pd.DataFrame.from_dict(tabla_tmin_fila, orient='index').T\n",
    "\n",
    "        # Concatenar los DataFrames\n",
    "        tabla_cc_tmax_observado = pd.concat([tabla_cc_tmax_observado, tabla_tmax_fila], ignore_index=True)\n",
    "        tabla_cc_tmin_observado = pd.concat([tabla_cc_tmin_observado, tabla_tmin_fila], ignore_index=True)\n",
    "\n",
    "        # Guardar la clim_observado de cc y data_modelo en un archivo CSV y TXT\n",
    "        tabla_cc_tmax_observado.to_csv(f\"{carpeta}/tabla_cc_tmax_observado_{fecha.year}.csv\", index=False)\n",
    "        tabla_cc_tmax_observado.to_csv(os.path.join(carpeta, f\"tabla_cc_tmax_observado_{fecha.year}.txt\"), index=False)\n",
    "\n",
    "        tabla_cc_tmin_observado.to_csv(f\"{carpeta}/tabla_cc_tmin_observado_{fecha.year}.csv\", index=False)\n",
    "        tabla_cc_tmin_observado.to_csv(os.path.join(carpeta, f\"tabla_cc_tmin_observado_{fecha.year}.txt\"), index=False)\n",
    "        \n",
    "        # Guardar el DataFrame en un archivo CSV con el nombre generado\n",
    "        file.to_csv(os.path.join(carpeta_observado, 'Editado', f\"{date}.temperatura.csv\"), index=False)\n",
    "        file.to_csv(os.path.join(carpeta_observado, 'Editado', f\"{date}.temperatura.txt\"), index=False)\n",
    "else:\n",
    "        print(\"No se pudieron obtener los datos correctamente ya que el archivo que contiene los datos de pronostico (data_modelo) está vacio.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3952d617",
   "metadata": {},
   "source": [
    "## Pasos previos al Ajuste de Datos con Climatología Mensual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfee07ff",
   "metadata": {},
   "source": [
    "### Crear un MAPEO que permite relacionar las cabeceras de cantón con estaciones especificas\n",
    "\n",
    "* Se define un diccionario llamado `mapeo`. Cada clave del diccionario representa el nombre de un cantón de Costa Rica, y el valor asociado a esa clave es una lista de estaciones que se asocian por ubicación a ese cantón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "43d8b14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeo = {\n",
    "    'ALAJUELA': ['84185', '84187'],\n",
    "    'ABANGARES': ['78031'],\n",
    "    'ACOSTA': ['88055'],\n",
    "    'AGUIRRE': ['90015'],\n",
    "    'ALAJUELITA': ['84203'],\n",
    "    'ALFARO_RUIZ': ['69737'],\n",
    "    'ALVARADO': ['73141'],\n",
    "    'ASERRI': ['84215', '84217'],\n",
    "    'ATENAS': ['84225'],\n",
    "    'BAGACES': ['76055'],\n",
    "    'BARVA': ['84243', '84197'],\n",
    "    'BELEN': ['84199'],\n",
    "    'BUENOS_AIRES': ['98087'],\n",
    "    'CANAS': ['76055'],\n",
    "    'CARRILLO': ['74067'],\n",
    "    'CARTAGO': ['73123'],\n",
    "    'CD_QUESADA': ['69743', '69701'],\n",
    "    'CORREDORES': ['100651'],\n",
    "    'COTO_BRUS': ['98107', '98109', '98075'],\n",
    "    'CURRIDABAT': ['84203'],\n",
    "    'DESAMPARADOS': ['84203'],\n",
    "    'DOTA': ['88051'],\n",
    "    'EL_GUARCO': ['73123', '73171'],\n",
    "    'ESPARZA': ['80013'],\n",
    "    'ESCAZU': ['84231', '84193'],\n",
    "    'FLORES': ['84199', '84197'],\n",
    "    'GARABITO': ['86013'],\n",
    "    'GOICOECHEA': ['84139'],\n",
    "    'GOLFITO': ['100643'],\n",
    "    'GRECIA': ['84295'],\n",
    "    'GUACIMO': ['73145'],\n",
    "    'GUAPILES': ['73147'],\n",
    "    'GUATUSO': ['69747', '69749'],\n",
    "    'HEREDIA': ['84193', '84243', '84197'],\n",
    "    'HOJANCHA': ['72185'],\n",
    "    'JIMENEZ': ['73149'],\n",
    "    'LA_CRUZ': ['72191'],\n",
    "    'LA_UNION': ['84181', '73129', '84249'],\n",
    "    'LEON_CORTEZ': ['88051', '88047'],\n",
    "    'LIBERIA': ['74063'],\n",
    "    'LIMON': ['81005'],\n",
    "    'LOS_CHILES': ['69633', '69713'],\n",
    "    'MATINA': ['73159', '83007'],\n",
    "    'MONTES_DE_OCA': ['84139', '84203'],\n",
    "    'MONTES_DE_ORO': ['78033'],\n",
    "    'MORA': ['84209', '84283'],\n",
    "    'MORAVIA': ['84139'],\n",
    "    'NANDAYURE': ['72189'],\n",
    "    'NARANJO': ['84239'],\n",
    "    'NICOYA': ['72165', '72183'],\n",
    "    'OREAMUNO': ['73123', '73129'],\n",
    "    'OROTINA': ['82011', '82019', '82017'],\n",
    "    'OSA': ['100655'],\n",
    "    'PALMARES': ['84239'],\n",
    "    'PARAISO': ['73123'],\n",
    "    'PARRITA': ['88049'],\n",
    "    'PAVAS': ['84193', '84285'],\n",
    "    'PEREZ_ZELEDON': ['98097', '94015'],\n",
    "    'POAS': ['84189', '84187'],\n",
    "    'POCOCI': ['73147', '73169'],\n",
    "    'PTO_VIEJO': ['85025', '85023'],\n",
    "    'PUNTARENAS': ['78027'],\n",
    "    'PURISCAL': ['84209'],\n",
    "    'QUEPOS': ['90015'],\n",
    "    'SAN_CARLOS': ['69743', '69701'],\n",
    "    'SAN_ISIDRO': ['84243'],\n",
    "    'SAN_JOSE': ['84141'],\n",
    "    'SAN_MARCOS': ['88051'],\n",
    "    'SAN_MATEO': ['82011'],\n",
    "    'SAN_PABLO': ['84243', '84193'],\n",
    "    'SAN_RAFAEL': ['84243'],\n",
    "    'SAN_RAMON': ['84239'],\n",
    "    'SANTA_ANA': ['84219'],\n",
    "    'SANTA_BARBARA': ['84197'],\n",
    "    'SANTA_CRUZ': ['74053'],\n",
    "    'SANTO_DOMINGO': ['84193'],\n",
    "    'SARAPIQUI': ['69725'],\n",
    "    'SIQUIRRES': ['73159'],\n",
    "    'SIXAOLA': ['87013'],\n",
    "    'TALAMANCA': ['85021'],\n",
    "    'TARRAZU': ['88051'],\n",
    "    'TIBAS': ['84141'],\n",
    "    'TILARAN': ['76063'],\n",
    "    'TORTUGUERO': ['71023'],\n",
    "    'TURRIALBA': ['73151', '73155', '73167'],\n",
    "    'TURRUBARES': ['84225'],\n",
    "    'UPALA': ['69679', '69647'],\n",
    "    'VALVERDE_VEGA': ['84239'],\n",
    "    'VASQUEZ': ['84207', '84213']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd9a88f",
   "metadata": {},
   "source": [
    "### Mapeo Invertido\n",
    "\n",
    "Se construye un diccionario inverso llamado `mapeo_invertido`. Inicialmente, se tiene un diccionario `mapeo` que asocia cantones con estaciones meteorológicas. \n",
    "\n",
    "El código realiza un bucle sobre cada elemento del diccionario `mapeo`, en cada iteración para cada estación asociado a un canton, se verifica si dicho código ya está presente en el diccionario inverso `mapeo_invertido`. Si la estación ya existe, se agrega el cantón actual a la lista existente de cantones asociadas a esa estación. En caso contrario, se crea una nueva entrada en `mapeo_invertido` con la estación como clave y una lista que contiene el cantón actual como único elemento.\n",
    "\n",
    "El resultado final es un diccionario inverso que asocia cada estación con una lista de cantones de Costa Rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6f85391f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapeo_invertido = {}\n",
    "\n",
    "for ubicacion, codigos in mapeo.items():\n",
    "    for codigo in codigos:\n",
    "        if codigo in mapeo_invertido:\n",
    "            mapeo_invertido[codigo].append(ubicacion)\n",
    "        else:\n",
    "            mapeo_invertido[codigo] = [ubicacion]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07448983",
   "metadata": {},
   "source": [
    "### Lista de nombres de las cabeceras de cantón\n",
    "\n",
    "* Crear una lista llamada `names_cantones` que contiene los nombres de cada cantón acomodados en el mismo orden que las columnas del diccionario `clim_modelo`. Lo anterior permitirá renombrar las columnas de `clim_modelo` luego para que tengan un formato correcto y homogeneo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "ad2637af",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_cantones = ['ALAJUELA', 'ABANGARES', 'ACOSTA', 'AGUIRRE', 'ALAJUELITA','ALFARO_RUIZ', 'ALVARADO', 'ASERRI', 'ATENAS', \n",
    "           'BAGACES', 'BARVA','BELEN', 'BUENOS_AIRES', 'CANAS', 'CARRILLO', 'CARTAGO', 'CD_QUESADA','CORREDORES', \n",
    "           'COTO_BRUS', 'CURRIDABAT', 'DESAMPARADOS', 'DOTA','EL_GUARCO', 'ESPARZA', 'ESCAZU', 'FLORES', 'GARABITO', \n",
    "           'GOICOECHEA','GOLFITO', 'GRECIA', 'GUACIMO', 'GUAPILES', 'GUATUSO', 'HEREDIA','HOJANCHA', 'JIMENEZ', 'LA_CRUZ', \n",
    "           'LA_UNION', 'LEON_CORTEZ', 'LIBERIA','LIMON', 'LOS_CHILES', 'MATINA', 'MONTES_DE_OCA', 'MONTES_DE_ORO','MORA', \n",
    "           'MORAVIA', 'NANDAYURE', 'NARANJO', 'NICOYA', 'OREAMUNO','OROTINA', 'OSA', 'PALMARES', 'PARAISO', 'PARRITA', \n",
    "           'PAVAS','PEREZ_ZELEDON', 'POAS', 'POCOCI', 'PTO_VIEJO', 'PUNTARENAS','PURISCAL', 'QUEPOS', 'SAN_CARLOS', \n",
    "           'SAN_ISIDRO', 'SAN_JOSE','SAN_MARCOS', 'SAN_MATEO', 'SAN_PABLO', 'SAN_RAFAEL', 'SAN_RAMON','SANTA_ANA', \n",
    "           'SANTA_BARBARA', 'SANTA_CRUZ', 'SANTO_DOMINGO','SARAPIQUI', 'SIQUIRRES', 'SIXAOLA', 'TALAMANCA', 'TARRAZU', \n",
    "           'TIBAS','TILARAN', 'TORTUGUERO', 'TURRIALBA', 'TURRUBARES', 'UPALA', 'VALVERDE_VEGA', 'VASQUEZ']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb6ecd",
   "metadata": {},
   "source": [
    "###  Integración y Promedio de datos por Estación usando el Mapeo Invertido\n",
    "\n",
    "Se crea la función `process_clim_modelo_tabla` se realiza para dar formato a los datos promedio de la climatología mensual del modelo. Primero, la función toma la columna deseada (`tabla`) del DataFrame `clim_modelo` y la almacena en un nuevo DataFrame llamado `df`.\n",
    "\n",
    "Posteriormente, crea un nuevo DataFrame llamado `Clim_Modelo_Mean` que servirá para almacenar los resultados finales procesados. La función itera sobre las columnas del diccionario inverso `mapeo_invertido`, el cual asocia cada estación meteorológica con la lista de cantones correspondientes.\n",
    "\n",
    "En cada iteración, verifica si la lista de cantones asociada a una estación tiene solo un elemento. Si es así, asigna directamente los valores de la columna correspondiente de `df` a esa estación en `Clim_Modelo_Mean`. Si la lista tiene más de un elemento, calcula la media de las columnas asociadas para esa estación, manejando los valores nulos.\n",
    "\n",
    "Finalmente, retorna el DataFrame resultante. Este proceso se utiliza para procesar dos conjuntos de datos de restas climatológicas correspondientes a las temperaturas máximas y mínimas, creando los DataFrames `Clim_Modelo_Tmax_Mean` y `Clim_Modelo_Tmin_Mean`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "32f437e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clim_modelo_tabla(tabla, names_cantones, mapeo_invertido):\n",
    "    # Convertir el DataFrame 'resta' a una nueva tabla llamada Tabla_Resta_Clim\n",
    "    df = clim_modelo[tabla]\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.drop('mes', axis=1)\n",
    "    df.columns = names_cantones\n",
    "\n",
    "    # Crear un nuevo DataFrame para almacenar los resultados finales\n",
    "    Clim_Modelo_Mean = pd.DataFrame()\n",
    "\n",
    "    # Iterar sobre las columnas de mapeo_invertido\n",
    "    for estacion, cantones in mapeo_invertido.items():\n",
    "        # Verificar si hay un solo elemento en la lista\n",
    "        if len(cantones) == 1:\n",
    "            Clim_Modelo_Mean[estacion] = df[cantones[0]]  # Asignar directamente el valor correspondiente\n",
    "        else:\n",
    "            # Si hay más de un elemento, calcular la media y manejar valores nulos\n",
    "            Clim_Modelo_Mean[estacion] = df[cantones].mean(axis=1, skipna=True)\n",
    "\n",
    "    return Clim_Modelo_Mean\n",
    "\n",
    "Clim_Modelo_Tmax_Mean = process_clim_modelo_tabla('tmax_mean', names_cantones, mapeo_invertido)\n",
    "Clim_Modelo_Tmin_Mean = process_clim_modelo_tabla('tmin_mean', names_cantones, mapeo_invertido)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01f5085",
   "metadata": {},
   "source": [
    "### Obtener la diferencia entre la climatología del modelo y la observada para cada cantón.\n",
    "\n",
    "La función `process_clim_resta_tabla`  realiza una operación entre `Clim_Modelo_Mean` y `Clim_Observ_Mean`, y luego agrega la columna 'mes' del DataFrame `clim_observado` al DataFrame resultante llamado `Clim_Resta`. La resta se realiza elemento por elemento entre las dos tablas de datos. Luego, la funcion se llama para crear dos dataframes de temperatura máxima y mínima, creando los DataFrames `Clim_Resta_Tmax` y `Clim_Resta_Tmin`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "ed9fb023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_clim_resta_tabla(Clim_Modelo_Mean, clim_observado, tabla):\n",
    "    \n",
    "    Clim_Observ_Mean = clim_observado[tabla]\n",
    "\n",
    "    Clim_Resta = pd.DataFrame()\n",
    "\n",
    "    for estacion in Clim_Modelo_Mean.columns:  # Corregir el nombre de la variable\n",
    "        resta = Clim_Modelo_Mean[estacion] - Clim_Observ_Mean[estacion]\n",
    "        Clim_Resta[estacion] = resta  # Usar directamente el DataFrame para asignar columnas\n",
    "\n",
    "    # Agregar la columna 'mes'\n",
    "    Clim_Resta['mes'] = clim_observado[tabla]['mes']\n",
    "    \n",
    "    return Clim_Resta\n",
    "\n",
    "Clim_Resta_Tmax = process_clim_resta_tabla(Clim_Modelo_Tmax_Mean, clim_observado, 'tmax_mean')\n",
    "Clim_Resta_Tmin = process_clim_resta_tabla(Clim_Modelo_Tmin_Mean, clim_observado, 'tmin_mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9abcc78",
   "metadata": {},
   "source": [
    "## Ajuste de Datos con Climatología Mensual Observada y Modelada\n",
    "\n",
    "El fragmento de código realiza una serie de procesamientos y ajustes en los datos de temperatura pronosticada (`Tmax` y `Tmin`) contenidos en el archivo `data_modelo`. Primero, se verifica si `data_modelo` no es `None`. Si hay datos disponibles, se procede a un bucle que itera sobre las fechas (`date`) y los archivos (`file`) en `file_date` y `data_modelo`, respectivamente.\n",
    "\n",
    "Para cada fecha, se convierte la cadena de fecha en un objeto `datetime`. Luego, se construyen las rutas completas para dos archivos CSV que contendrán tablas de ajuste para temperaturas máximas (`tabla_ajuste_tmax`) y mínimas (`tabla_ajuste_tmin`). Si los archivos ya existen, se cargan en DataFrames (`tabla_ajuste_tmax` y `tabla_ajuste_tmin`). En caso contrario, se crean DataFrames vacíos con columnas específicas.\n",
    "\n",
    "Se verifica si la fecha actual ya existe en las tablas de ajuste. Si es así, se eliminan las filas existentes correspondientes. Se crea una nueva fila en las tablas de ajuste con la fecha actual y se itera sobre las estaciones meteorológicas presentes en las tablas de ajuste.\n",
    "\n",
    "Para cada estación, se verifica si existe en las nuevas tablas de temperatura (`nueva_tabla_tmax` y `nueva_tabla_tmin`). Si la estación está presente, se calcula el valor de ajuste restando la diferencia entre la climatología modelada y observada. Este valor ajustado se utiliza para actualizar los datos en `data_modelo` y se agrega a la fila correspondiente en las tablas de ajuste. Si la estación no está presente, se asigna \"x\" a la columna correspondiente.\n",
    "\n",
    "Finalmente, se guardan las tablas de ajuste en archivos CSV y TXT, y se guarda el DataFrame actualizado en un archivo CSV específico para la fecha actual en la carpeta `WRF_Ajust_Clim` dentro de `carpeta_modelo`. Si `data_modelo` es `None`, se imprime un mensaje indicando que no se pudieron obtener los datos correctamente. En resumen, este código realiza ajustes en las temperaturas pronosticadas y guarda los resultados en tablas de ajuste y archivos específicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "7bbece63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La estación '69699' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69709' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69711' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69715' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69717' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69721' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69723' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69729' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69731' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69735' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '71015' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '72159' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '72163' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '72167' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '72181' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '72195' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '73137' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '73153' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '74051' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '74071' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '74081' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '76059' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '76061' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '78035' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '82013' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '82015' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '84251' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '94013' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '96003' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '98091' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '100641' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '100649' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '100653' no existe en la nueva tabla de tmax en la fecha 20231204.\n",
      "La estación '69699' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69709' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69711' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69715' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69717' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69721' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69723' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69729' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69731' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69735' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '71015' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '72159' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '72163' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '72167' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '72181' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '72195' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '73137' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '73153' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '74051' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '74071' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '74081' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '76059' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '76061' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '78035' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '82013' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '82015' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '84251' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '94013' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '96003' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '98091' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '100641' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '100649' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '100653' no existe en la nueva tabla de tmin en la fecha 20231204.\n",
      "La estación '69699' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69709' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69711' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69715' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69717' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69721' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69723' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69729' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69731' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69735' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '71015' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '72159' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '72163' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '72167' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '72181' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '72195' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '73137' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '73153' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '74051' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '74071' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '74081' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '76059' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '76061' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '78035' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '82013' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '82015' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '84251' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '94013' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '96003' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '98091' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '100641' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '100649' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '100653' no existe en la nueva tabla de tmax en la fecha 20231205.\n",
      "La estación '69699' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69709' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69711' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69715' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69717' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69721' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69723' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69729' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69731' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69735' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '71015' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '72159' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '72163' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '72167' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '72181' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '72195' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '73137' no existe en la nueva tabla de tmin en la fecha 20231205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La estación '73153' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '74051' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '74071' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '74081' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '76059' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '76061' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '78035' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '82013' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '82015' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '84251' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '94013' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '96003' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '98091' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '100641' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '100649' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '100653' no existe en la nueva tabla de tmin en la fecha 20231205.\n",
      "La estación '69699' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69709' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69711' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69715' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69717' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69721' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69723' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69729' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69731' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69735' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '71015' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '72159' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '72163' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '72167' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '72181' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '72195' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '73137' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '73153' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '74051' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '74071' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '74081' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '76059' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '76061' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '78035' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '82013' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '82015' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '84251' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '94013' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '96003' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '98091' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '100641' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '100649' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '100653' no existe en la nueva tabla de tmax en la fecha 20231206.\n",
      "La estación '69699' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69709' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69711' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69715' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69717' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69721' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69723' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69729' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69731' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '69735' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '71015' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '72159' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '72163' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '72167' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '72181' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '72195' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '73137' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '73153' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '74051' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '74071' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '74081' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '76059' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '76061' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '78035' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '82013' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '82015' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '84251' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '94013' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '96003' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '98091' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '100641' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '100649' no existe en la nueva tabla de tmin en la fecha 20231206.\n",
      "La estación '100653' no existe en la nueva tabla de tmin en la fecha 20231206.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si data_modelo es None\n",
    "if data_modelo is not None:\n",
    "    for date, file in zip(file_date, data_modelo):\n",
    "\n",
    "        fecha = datetime.strptime(date, \"%Y%m%d\")\n",
    "        \n",
    "        # Ruta completa del archivo que contendrá la tabla de ajuste\n",
    "        ruta_ajuste_tmax = os.path.join(carpeta, f\"tabla_ajuste_tmax_{fecha.year}.csv\")\n",
    "        ruta_ajuste_tmin = os.path.join(carpeta, f\"tabla_ajuste_tmin_{fecha.year}.csv\")\n",
    "\n",
    "        # Si el archivo no existe, se crea la tabla vacía y se descargan y procesan los datos\n",
    "        if os.path.exists(ruta_ajuste_tmax) and os.path.exists(ruta_ajuste_tmin):\n",
    "            # Continuar con el procesamiento\n",
    "            tabla_ajuste_tmax = pd.read_csv(ruta_ajuste_tmax)\n",
    "            tabla_ajuste_tmin = pd.read_csv(ruta_ajuste_tmin)\n",
    "        else:\n",
    "            # Si el archivo existe, cargar la tabla y procesar el día siguiente\n",
    "            tabla_ajuste_tmax = pd.DataFrame(columns=['FECHA'] + [str(col) for col in file['Estacion']])\n",
    "            tabla_ajuste_tmin = pd.DataFrame(columns=['FECHA'] + [str(col) for col in file['Estacion']])\n",
    "        \n",
    "        # Convertir fecha_siguiente_str a np.int64 para hacer la comparación correctamente\n",
    "        fecha_siguiente_int = np.int64(date)\n",
    "\n",
    "        # Verificar si la fecha ya existe en la tabla\n",
    "        if (tabla_ajuste_tmax[\"FECHA\"] == fecha_siguiente_int).any() or (tabla_ajuste_tmin[\"FECHA\"] == fecha_siguiente_int).any():\n",
    "            # Si la fecha ya existe, obtener el índice de la fila existente\n",
    "            idx_fecha_existente_tmax = tabla_ajuste_tmax.loc[tabla_ajuste_tmax[\"FECHA\"] == fecha_siguiente_int].index[0]\n",
    "            idx_fecha_existente_tmin = tabla_ajuste_tmin.loc[tabla_ajuste_tmin[\"FECHA\"] == fecha_siguiente_int].index[0]\n",
    "\n",
    "            # Eliminar la fila existente a partir del índice\n",
    "            if idx_fecha_existente_tmax is not None:\n",
    "                tabla_ajuste_tmax = tabla_ajuste_tmax.drop(idx_fecha_existente_tmax)\n",
    "\n",
    "            if idx_fecha_existente_tmin is not None:\n",
    "                tabla_ajuste_tmin = tabla_ajuste_tmin.drop(idx_fecha_existente_tmin)\n",
    "\n",
    "        # Crear una nueva fila con la fecha actual\n",
    "        tabla_tmax_fila = {col: \"\" for col in tabla_ajuste_tmax.columns}\n",
    "        tabla_tmax_fila[\"FECHA\"] = date\n",
    "\n",
    "        tabla_tmin_fila = {col: \"\" for col in tabla_ajuste_tmin.columns}\n",
    "        tabla_tmin_fila[\"FECHA\"] = date\n",
    "\n",
    "        for estacion in tabla_ajuste_tmax.columns[1:]:\n",
    "            if estacion in Clim_Resta_Tmax.columns:\n",
    "                # Filtrar según el valor de 'mes'\n",
    "                valor_resta_tmax = Clim_Resta_Tmax.loc[Clim_Resta_Tmax[\"mes\"] == fecha.month, estacion].values[0]\n",
    "\n",
    "                # Verificar si existen filas que coincidan con la estación en data_modelo\n",
    "                filas_estacion = file.loc[file[\"Estacion\"].astype(str).str.strip() == estacion.strip()]\n",
    "\n",
    "                # Verificar que la fila para la 'estación' en temperatura pronosticada no esté vacía\n",
    "                if not filas_estacion.empty:\n",
    "                    # Obtener el valor de temperatura pronosticada para la estación y el mes correspondiente\n",
    "                    valor_modelo_tmax = filas_estacion[\"Tmax\"].iloc[0]\n",
    "\n",
    "                    # Ajustar el valor de temperatura pronosticada restando la diferencia entre la climatología modelada y observada\n",
    "                    nuevo_valor_tmax = valor_modelo_tmax - valor_resta_tmax\n",
    "\n",
    "                    # Actualizar el valor en data_modelo con el ajuste realizado\n",
    "                    file.loc[filas_estacion.index, \"Tmax\"] = nuevo_valor_tmax\n",
    "\n",
    "                    # Añadir el nuevo valor ajustado a la nueva fila\n",
    "                    tabla_tmax_fila[estacion] = f\"{nuevo_valor_tmax}({valor_modelo_tmax})\"\n",
    "\n",
    "                else:\n",
    "                    # Si no hay valor para la estación en data_modelo, asignar \"x\"\n",
    "                    tabla_tmax_fila[estacion] = \"x\"\n",
    "            else:\n",
    "                print(f\"La estación '{estacion}' no existe en la nueva tabla de tmax en la fecha {date}.\")\n",
    "                tabla_tmax_fila[estacion] = \"-\"\n",
    "\n",
    "        for estacion in tabla_ajuste_tmin.columns[1:]:\n",
    "            if estacion in Clim_Resta_Tmin.columns:\n",
    "                # Filtrar según el valor de 'mes'\n",
    "                valor_resta_tmin = Clim_Resta_Tmin.loc[Clim_Resta_Tmin[\"mes\"] == fecha.month, estacion].values[0]\n",
    "\n",
    "                # Verificar si existen filas que coincidan con la estación en data_modelo\n",
    "                filas_estacion = file.loc[file[\"Estacion\"].astype(str).str.strip() == estacion.strip()]\n",
    "\n",
    "                # Verificar que la fila para la 'estación' en temperatura pronosticada no esté vacía\n",
    "                if not filas_estacion.empty:\n",
    "                    # Obtener el valor de temperatura pronosticada para la estación y el mes correspondiente\n",
    "                    valor_modelo_tmin = filas_estacion[\"Tmin\"].iloc[0]\n",
    "\n",
    "                    # Ajustar el valor de temperatura pronosticada restando la diferencia entre la climatología modelada y observada\n",
    "                    nuevo_valor_tmin = valor_modelo_tmin - valor_resta_tmin\n",
    "\n",
    "                    # Actualizar el valor en data_modelo con el ajuste realizado\n",
    "                    file.loc[filas_estacion.index, \"Tmin\"] = nuevo_valor_tmin\n",
    "\n",
    "                    # Añadir el nuevo valor ajustado a la nueva fila\n",
    "                    tabla_tmin_fila[estacion] = f\"{nuevo_valor_tmin}({valor_modelo_tmin})\"\n",
    "\n",
    "                else:\n",
    "                    # Si no hay valor para la estación en data_modelo, asignar \"x\"\n",
    "                    tabla_tmin_fila[estacion] = \"x\"\n",
    "            else:\n",
    "                print(f\"La estación '{estacion}' no existe en la nueva tabla de tmin en la fecha {date}.\")\n",
    "                tabla_tmin_fila[estacion] = \"-\"\n",
    "\n",
    "        # Convert the dictionaries into Pandas DataFrames\n",
    "        tabla_tmax_fila = pd.DataFrame.from_dict(tabla_tmax_fila, orient='index').T\n",
    "        tabla_tmin_fila = pd.DataFrame.from_dict(tabla_tmin_fila, orient='index').T\n",
    "\n",
    "        # Concatenate the DataFrames\n",
    "        tabla_ajuste_tmax = pd.concat([tabla_ajuste_tmax, tabla_tmax_fila], ignore_index=True)\n",
    "        tabla_ajuste_tmin = pd.concat([tabla_ajuste_tmin, tabla_tmin_fila], ignore_index=True)\n",
    "\n",
    "        # Guardar la tabla ajustada en un archivo CSV y TXT\n",
    "        tabla_ajuste_tmax.to_csv(os.path.join(carpeta, f\"tabla_ajuste_tmax_{fecha.year}.csv\"), index=False)\n",
    "        tabla_ajuste_tmax.to_csv(os.path.join(carpeta, f\"tabla_ajuste_tmax_{fecha.year}.txt\"), index=False)\n",
    "        \n",
    "        tabla_ajuste_tmin.to_csv(os.path.join(carpeta, f\"tabla_ajuste_tmin_{fecha.year}.csv\"), index=False)\n",
    "        tabla_ajuste_tmin.to_csv(os.path.join(carpeta, f\"tabla_ajuste_tmin_{fecha.year}.txt\"), index=False)\n",
    "        \n",
    "        # Guardar el DataFrame en un archivo CSV con el nombre generado\n",
    "        file.to_csv(os.path.join(carpeta_modelo, 'WRF_Ajust_Clim', f\"{date}.temperatura.csv\"), index=False)\n",
    "        file.to_csv(os.path.join(carpeta_modelo, 'WRF_Ajust_Clim', f\"{date}.temperatura.txt\"), index=False)\n",
    "\n",
    "else:\n",
    "    print(\"No se pudieron obtener los datos correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db479dbf",
   "metadata": {},
   "source": [
    "## Ajuste de datos con el promedio de la resta de los 3 días anteriores\n",
    "\n",
    "### Crear o Abrir Tabla de Promedios de Resta de los 3 días anteriores\n",
    "\n",
    "Se genera una fecha para el dia siguiente (`fecha`), una fecha anterior (hoy) (`fecha_anterior`), y se extrae el año correspondiente (`ano`). Se define el nombre de la tabla que se creará o abrirá para contener el promedio de la diferencia entre pronóstico y observado. Luego, se verifica si la carpeta especificada existe; si no, se crea. Se construye la ruta completa del archivo de la tabla, y si el archivo no existe, se inicializa la tabla vacía con la columna 'Estacion' del primer conjunto de datos en `data_modelo`. En caso de que `data_modelo` sea `None`, se imprime un mensaje de error. Si el archivo ya existe, los datos se cargan desde el archivo CSV. Se realiza el procedimiento para temperatura máxima y temperatura minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "9800439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la fecha actual y la fecha anterior\n",
    "fecha = datetime.now().date() + timedelta(days=1)\n",
    "fecha_anterior = datetime.now().date()\n",
    "\n",
    "# Obtener el año de la fecha actual\n",
    "ano = fecha.year\n",
    "\n",
    "# Nombre de las tablas que contendrán el promedio de la diferencia (pronóstico - observado)\n",
    "nombre_tabla_tmax = f\"tabla_promedios_tmax_{ano}\"\n",
    "nombre_tabla_tmin = f\"tabla_promedios_tmin_{ano}\"\n",
    "\n",
    "# Rutas completas de los archivos CSV que contendrán las tablas\n",
    "ruta_archivo_tmax = os.path.join(carpeta, f\"{nombre_tabla_tmax}.csv\")\n",
    "ruta_archivo_tmin = os.path.join(carpeta, f\"{nombre_tabla_tmin}.csv\")\n",
    "\n",
    "# Si los archivos existen, cargar los datos de las tablas\n",
    "if os.path.exists(ruta_archivo_tmax) and os.path.exists(ruta_archivo_tmin):\n",
    "    tabla_promedios_tmax = pd.read_csv(ruta_archivo_tmax)\n",
    "    tabla_promedios_tmin = pd.read_csv(ruta_archivo_tmin)\n",
    "else:\n",
    "    # Si los archivos no existen y hay datos_modelo, crear tablas con la columna \"Estacion\"\n",
    "    if data_modelo is not None:\n",
    "        tabla_promedios_tmax = data_modelo[0][[\"Estacion\"]]\n",
    "        tabla_promedios_tmin = data_modelo[0][[\"Estacion\"]]\n",
    "    else:\n",
    "        print(\"No se pudieron obtener los datos correctamente.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2605c97c",
   "metadata": {},
   "source": [
    "### Verificar si hay datos del pronóstico ajustados para los 3 dían anteriores\n",
    "\n",
    "Se especifica la ruta de dos carpetas. La primera carpeta, \"carpeta_wrf_ajust,\" contiene datos ajustados del modelo WRF, mientras que la segunda carpeta, \"carpeta_obs_crudo,\" almacena datos observados.\n",
    "\n",
    "La ejecución del código está condicionada a la existencia y validez de la carpeta \"carpeta_wrf_ajust.\" Si la carpeta existe y es un directorio válido, el programa procede a verificar la cantidad de archivos en dicha carpeta. Si hay al menos 8 archivos, se inicia un proceso de carga de datos.\n",
    "\n",
    "Para cada fecha en la lista de fechas de pronóstico (\"file_date\"), el código construye las rutas completas de los archivos de pronóstico y observados en las carpetas correspondientes. Si ambos archivos existen, se leen en pandas DataFrames y se agregan a las listas \"data_modelo\" y \"data_observada\" respectivamente. También, la fecha se agrega a la lista \"dates.\"\n",
    "\n",
    "En caso de que un archivo no se encuentre en las carpetas especificadas, se imprime un mensaje indicando la ausencia del archivo para esa fecha.\n",
    "\n",
    "Si la carpeta \"carpeta_wrf_ajust\" tiene menos de 8 archivos, se imprime un mensaje indicando la insuficiencia de archivos y se informa que no se cargarán datos ajustados del WRF. Si la carpeta no existe o no es un directorio válido, se emiten mensajes informando de esta situación y también se indica que no se cargarán archivos ajustados del WRF.\n",
    "\n",
    "Finalmente, si las variables \"file_date\" o \"data_modelo\" no contienen datos, se imprime un mensaje indicando que una o ambas tablas están vacías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "555b9890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La carpeta tiene 8 o más archivos. Cantidad de archivos: 28\n"
     ]
    }
   ],
   "source": [
    "# Especificar la ruta de la carpeta que contiene los datos ajustados del modelo WRF\n",
    "carpeta_wrf_ajust = f\"{carpeta}/Modelo/WRF_Ajustado/\"\n",
    "carpeta_obs_crudo = f\"{carpeta}/Observado/Crudo/\"\n",
    "\n",
    "if file_date and data_modelo:\n",
    "    # Verificar si la carpeta existe y es un directorio válido\n",
    "    if os.path.exists(carpeta_wrf_ajust) and os.path.isdir(carpeta_wrf_ajust):\n",
    "        # Obtener la lista de archivos en la carpeta\n",
    "        archivos_en_carpeta = os.listdir(carpeta_wrf_ajust)\n",
    "\n",
    "        # Contar la cantidad de archivos en la carpeta\n",
    "        cantidad_archivos = len(archivos_en_carpeta)\n",
    "\n",
    "        # Verificar si hay 8 o más archivos en la carpeta\n",
    "        if cantidad_archivos >= 8:\n",
    "            print(f\"La carpeta tiene 8 o más archivos. Cantidad de archivos: {cantidad_archivos}\")\n",
    "            data_modelo, data_observada, dates = [], [], []\n",
    "\n",
    "            # Iterar sobre las fechas de pronóstico\n",
    "            for date in file_date:\n",
    "                # Crear la ruta completa del archivo de pronóstico en el directorio local\n",
    "                ruta_wrf_ajust = os.path.join(carpeta_wrf_ajust, f\"{date}.temperatura.csv\")\n",
    "                ruta_obs_crudo = os.path.join(carpeta_obs_crudo, f\"{date}.temperatura.csv\")\n",
    "\n",
    "                # Verificar si el archivo existe en la carpetas\n",
    "                if os.path.exists(ruta_wrf_ajust) and os.path.exists(ruta_obs_crudo):\n",
    "                    # Leer el archivo CSV y agregar los datos al conjunto de datos 'data_modelo'\n",
    "                    file_wrf = pd.read_csv(ruta_wrf_ajust)\n",
    "                    data_modelo.append(file_wrf)\n",
    "                    # Leer el archivo CSV y agregar los datos al conjunto de datos 'data_observada'\n",
    "                    file_obs = pd.read_csv(ruta_obs_crudo)\n",
    "                    data_observada.append(file_obs)\n",
    "                    # Agregar las fechas de datos a 'dates'\n",
    "                    dates.append(date)\n",
    "                else:\n",
    "                    # Imprimir un mensaje si el archivo no se encuentra en la carpeta de datos ajustados del WRF\n",
    "                    print(f'El archivo para el {date} no se encuentra en la carpeta de datos del WRF ajustados o observados')\n",
    "        else:\n",
    "            # Imprimir un mensaje si la carpeta tiene menos de 8 archivos\n",
    "            print(f\"La carpeta tiene menos de 8 archivos. Cantidad de archivos: {cantidad_archivos}\")\n",
    "            print(\"No se cargan archivos del WRF ya ajustados\")\n",
    "    else:\n",
    "        # Imprimir un mensaje si la carpeta no existe o no es un directorio válido\n",
    "        print(\"La carpeta no existe o no es un directorio válido.\")\n",
    "        print(\"No se cargan archivos del WRF ya ajustados\")\n",
    "else:\n",
    "    print(\"Alguna de las tablas o ambas estan vacias\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62e4ac",
   "metadata": {},
   "source": [
    "### Obtener la resta (pronostico menos observado) de los 3 días anteriores\n",
    "\n",
    "En este fragmento de código, se crea una lista llamada `tablas_diferencias` destinada a almacenar los resultados de las diferencias entre los datos pronosticados y observados de temperaturas máximas y mínimas. Se verifica la presencia de datos tanto en `data_modelo` como en `data_observada`.\n",
    "\n",
    "En caso de que ambas listas contengan datos, se utiliza un bucle `for` para recorrer las listas de DataFrames correspondientes a los datos pronosticados (`data_modelo`) y observados (`data_observada`). Para cada iteración, se realiza una combinación de los DataFrames mediante la columna \"Estacion\" utilizando el método `merge`, y se calculan las diferencias entre las temperaturas pronosticadas y observadas para cada día. Estas diferencias se almacenan en las columnas \"Diferencia_Tmax\" y \"Diferencia_Tmin\".\n",
    "\n",
    "Posteriormente, se filtran las columnas para seleccionar únicamente las necesarias, que son \"Estacion\", \"Diferencia_Tmax\" y \"Diferencia_Tmin\". Se eliminan las filas con valores NaN. La tabla resultante se agrega a la lista `tablas_diferencias`.\n",
    "\n",
    "En el caso de que alguna de las listas (`data_modelo` o `data_observada`) esté vacía, se imprime un mensaje indicando que al menos una de las tablas está vacía o ambas.\n",
    "\n",
    "En resumen, este código calcula y almacena las diferencias entre las temperaturas pronosticadas y observadas para cada estación y día, y organiza estos resultados en una lista llamada `tablas_diferencias`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "4c7a540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear una lista para almacenar los resultados (temperatura pronosticada menos temperatura observada)\n",
    "tablas_diferencias = []\n",
    "\n",
    "if data_modelo and data_observada:\n",
    "    # Recorrer las listas de DataFrames (pro y obs) para calcular las diferencias (pro - obs) para cada día\n",
    "    for i in range(len(data_modelo)):\n",
    "\n",
    "        # Realizar una combinación \"left\" entre pro y obs en base a la columna \"Estacion\"\n",
    "        diferencias = data_modelo[i].merge(data_observada[i], on=\"Estacion\", how=\"left\", suffixes=(\"_pro\", \"_obs\"))\n",
    "\n",
    "        # Calcular las diferencias (pro - obs) y guardarlo en la columna 'Diferencia_Tmax' y 'Diferencia_Tmin'\n",
    "        diferencias[\"Diferencia_Tmax\"] = diferencias[\"Tmax_pro\"] - diferencias[\"Tmax_obs\"]\n",
    "        diferencias[\"Diferencia_Tmin\"] = diferencias[\"Tmin_pro\"] - diferencias[\"Tmin_obs\"]\n",
    "\n",
    "        # Filtrar para seleccionar solo las columnas necesarias\n",
    "        diferencias = diferencias[[\"Estacion\", \"Diferencia_Tmax\", \"Diferencia_Tmin\"]]\n",
    "        \n",
    "        # Eliminar filas con valores NaN en la columna 'Resta'\n",
    "        diferencias = diferencias.dropna(subset=[\"Diferencia_Tmax\", \"Diferencia_Tmin\"])\n",
    "\n",
    "        # Agregar la tabla diferencias a la lista tablas_diferencias\n",
    "        tablas_diferencias.append(diferencias)\n",
    "else:\n",
    "    print(\"Alguna de las tablas (datos_modelo y data_observada) o ambas estan vacias\")## Pronostico menos Observado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07419002",
   "metadata": {},
   "source": [
    "### Obtener el promedio de la resta de los 3 días anteriores\n",
    "\n",
    "Se utiliza `pd.concat()` para concatenar las tablas de diferencias presentes en la lista `tablas_diferencias` en una sola tabla llamada `tabla_concatenada`. Se calcula el promedio de las diferencias agrupando por estación para las temperaturas máximas (`tabla_promedio_tmax`) y mínimas (`tabla_promedio_tmin`). El método `groupby` se utiliza para agrupar por la columna \"Estacion\", y luego se aplica la función `mean()` para calcular el promedio.\n",
    "\n",
    "La columna de diferencias para temperaturas máximas se renombra como la fecha en formato texto (`text`) en `tabla_promedio_tmax`, y para temperaturas mínimas en `tabla_promedio_tmin`. Se verifica si la columna correspondiente a la fecha ya existe en las tablas `tabla_promedios_tmax` y `tabla_promedios_tmin`. En caso afirmativo, se elimina la columna existente.\n",
    "\n",
    "Se fusionan las tablas `tabla_promedios_tmax` y `tabla_promedios_tmin` con las nuevas tablas `tabla_promedio_tmax` y `tabla_promedio_tmin`, respectivamente. Se utiliza el método `merge` con la columna \"Estacion\" como clave y el modo \"outer\" para incluir todas las estaciones. Finalmente, se guardan las tablas resultantes (`tabla_promedios_tmax` y `tabla_promedios_tmin`) en archivos CSV y TXT en la carpeta especificada (`carpeta`).\n",
    "\n",
    "En caso de que la lista `tablas_diferencias` esté vacía, se imprime un mensaje indicando que la tabla de diferencias está vacía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f1803359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta  # Importamos clases relacionadas con fechas y horas.\n",
    "\n",
    "if tablas_diferencias:\n",
    "    # Concatenar las tres tablas de tablas_diferencias en una sola tabla\n",
    "    tabla_concatenada = pd.concat(tablas_diferencias)\n",
    "\n",
    "    # Calcular el promedio agrupando por Estación\n",
    "    tabla_promedio_tmax = tabla_concatenada.groupby(\"Estacion\")[\"Diferencia_Tmax\"].mean().reset_index()\n",
    "    tabla_promedio_tmin = tabla_concatenada.groupby(\"Estacion\")[\"Diferencia_Tmin\"].mean().reset_index()\n",
    "\n",
    "    # Obtener la fecha actual en formato texto y renombrar la columna 'Resta' de tabla_promedio\n",
    "    text = fecha.strftime(\"%Y%m%d\")\n",
    "    tabla_promedio_tmax = tabla_promedio_tmax.rename(columns={\"Diferencia_Tmax\": f\"{text}\"})\n",
    "    tabla_promedio_tmin = tabla_promedio_tmin.rename(columns={\"Diferencia_Tmin\": f\"{text}\"})\n",
    "\n",
    "    # Verificar si la columna ya existe en tabla_promedios_tmax, tabla_promedios_tmin\n",
    "    if f\"{text}\" in tabla_promedios_tmax.columns:\n",
    "        # Si existe, eliminarla\n",
    "        tabla_promedios_tmax.drop(columns=[f\"{text}\"], inplace=True)\n",
    "\n",
    "    if f\"{text}\" in tabla_promedios_tmin.columns:\n",
    "        # Si existe la columna existe, eliminarla\n",
    "        tabla_promedios_tmin.drop(columns=[f\"{text}\"], inplace=True)\n",
    "\n",
    "    # Fusionar tabla_promedios_tmax, tabla_promedios_tmin con tabla_promedio_tmax, tabla_promedio_tmin\n",
    "    tabla_promedios_tmax = tabla_promedios_tmax.merge(tabla_promedio_tmax, on=\"Estacion\", how=\"outer\")\n",
    "    tabla_promedios_tmin = tabla_promedios_tmin.merge(tabla_promedio_tmin, on=\"Estacion\", how=\"outer\")\n",
    "\n",
    "    # Guardar la tabla_promedios_tmax, tabla_promedios_tmin en un archivo CSV y TXT\n",
    "    tabla_promedios_tmax.to_csv(f\"{carpeta}/tabla_promedios_tmax_{ano}.csv\", index=False)\n",
    "    tabla_promedios_tmax.to_csv(os.path.join(carpeta, f\"tabla_promedios_tmax_{ano}.txt\"), index=False)\n",
    "\n",
    "    tabla_promedios_tmin.to_csv(f\"{carpeta}/tabla_promedios_tmin_{ano}.csv\", index=False)\n",
    "    tabla_promedios_tmin.to_csv(os.path.join(carpeta, f\"tabla_promedios_tmin_{ano}.txt\"), index=False)\n",
    "else:\n",
    "    print(\"Tabla diferencias está vacía\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd2c0fa",
   "metadata": {},
   "source": [
    "### Cargar pronóstico de mañana\n",
    "\n",
    "Se construye un enlace completo (`url_modelo_mañana`) para el archivo CSV del pronóstico de temperatura del día siguiente. El enlace se forma utilizando la base del enlace (`enlace_base_mod`), la fecha actual (`fecha_anterior`) y la fecha siguiente (`fecha`), todas formateadas en el formato adecuado.\n",
    "\n",
    "Se verifica la existencia del archivo en el enlace. Si el estado de la respuesta (`response.status_code`) es 200, significa que el archivo existe y se puede proceder con la descarga. Se descarga el archivo y se procesa para asegurar la integridad de los datos. Se comprueba si hay valores negativos o de tipo string en las columnas de temperaturas pronosticadas. Si se encuentran valores negativos o de tipo string, se imprime un mensaje indicando que no se carga el archivo.\n",
    "\n",
    "Se sustituyen los valores de tipo string por NaN en las columnas de temperaturas (`Tmax` y `Tmin`).Se guarda el DataFrame resultante en archivos CSV y TXT en la carpeta especificada (`carpeta_modelo`, en la subcarpeta `WRF_Crudo`). El nombre del archivo se genera utilizando la fecha actual (`fecha`) formateada correctamente.\n",
    "\n",
    "Si el estado de la respuesta no es 200, se imprime un mensaje indicando que hay archivos faltantes para la fecha actual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "cc14ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formar el enlace completo para el archivo CSV con la fecha actual y siguiente¿\n",
    "url_modelo_mañana = f\"{enlace_base_mod}{fecha_anterior.strftime('%Y%m%d')}/{fecha.strftime('%Y%m%d')}.temperatura.csv\"\n",
    "\n",
    "# Verificar si el archivo de temperatura observada existe antes de descargarlo\n",
    "response = requests.get(url_modelo_mañana)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    csv_data = response.content.decode('utf-8')\n",
    "    df = pd.read_csv(io.StringIO(csv_data), header=None, names=COLUMN_NAMES)\n",
    "\n",
    "    negative_values = (df[COLUMN_NAMES[1:]] < 0).any(axis=0)\n",
    "    str_values = df[COLUMN_NAMES[1:]].applymap(lambda x: isinstance(x, str)).any(axis=0)\n",
    "\n",
    "    if (negative_values.any() or str_values.any()):\n",
    "        print(f\"Valores de temperatura pronosticada para {fecha} son negativos o del tipo str. No se carga el archivo.\")\n",
    "    else:\n",
    "        # Sustituir los valores str por NaN en las columnas\n",
    "        df[[\"Tmax\", \"Tmin\"]] = df[[\"Tmax\", \"Tmin\"]].apply(pd.to_numeric, errors='coerce')\n",
    "        \n",
    "        file = df[['Estacion', \"Tmax\", \"Tmin\"]]\n",
    "\n",
    "        # Guardar el DataFrame en un archivo CSV con el nombre generado\n",
    "        file.to_csv(os.path.join(carpeta_modelo, 'WRF_Crudo', f\"{fecha.strftime('%Y%m%d')}.temperatura.csv\"), index=False)\n",
    "        file.to_csv(os.path.join(carpeta_modelo, 'WRF_Crudo', f\"{fecha.strftime('%Y%m%d')}.temperatura.txt\"), index=False)\n",
    "else:\n",
    "    print(f\"Hay archivos faltantes para la fecha {fecha.strftime('%Y%m%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62070ad6",
   "metadata": {},
   "source": [
    "### Restar al pronóstico de mañana el promedio de la resta de los 3 días previos\n",
    "\n",
    "Se verifica la existencia de tres variables (`tabla_promedio_tmax`, `tabla_promedio_tmin`, y `file`). Se utiliza la estructura condicional `if \"variable\" in locals()` para verificar si cada una de las tres variables (`tabla_promedio_tmax`, `tabla_promedio_tmin`, y `file`) existe en el entorno actual.\n",
    "\n",
    "Se verifica si las variables existen y no son nulas (`None`). Luego, se comprueba si las variables no están vacías mediante la condición `if not variable.empty`. Si todas las verificaciones son exitosas, se procede a realizar un ajuste al pronóstico de temperatura para el día siguiente. Se realiza la resta del pronóstico de mañana con el promedio de los tres días previos para las temperaturas máximas y mínimas.\n",
    "\n",
    "Se crea un nuevo DataFrame llamado `Pronostico` que contiene la estación y las columnas ajustadas de temperaturas máximas (`Tmax`) y mínimas (`Tmin`). Se guardan los resultados del ajuste en archivos CSV y TXT en la carpeta especificada (`carpeta`). El nombre del archivo se genera utilizando la fecha actual (`fecha`) formateada correctamente.\n",
    "\n",
    "Si alguna de las variables (`tabla_promedio_tmax`, `tabla_promedio_tmin`, y `file`) está vacía o no está definida, se imprime un mensaje indicando la situación correspondiente. Si alguna de las variables no existe en el entorno actual, se imprime un mensaje indicando la situación correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "c1df74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"tabla_promedio_tmax\" in locals() and \"tabla_promedio_tmin\" in locals() and \"file\" in locals():\n",
    "    if tabla_promedio_tmax is not None and tabla_promedio_tmin is not None and file is not None:\n",
    "        if not tabla_promedio_tmax.empty and not tabla_promedio_tmin.empty and not file.empty:\n",
    "            # Verificar si hay NaN en los DataFrames\n",
    "            if tabla_promedio_tmax.isnull().values.any() or tabla_promedio_tmin.isnull().values.any() or file.isnull().values.any():\n",
    "                print(\"Alguno de los DataFrames contiene valores NaN.\")\n",
    "            else:\n",
    "                # Restarle al pronóstico de mañana el promedio de los 3 días previos\n",
    "                Pronostico_tmax = file[[\"Estacion\", \"Tmax\"]]\n",
    "                Pronostico_tmin = file[[\"Estacion\", \"Tmin\"]]\n",
    "\n",
    "                merged_tmax = pd.merge(Pronostico_tmax, tabla_promedio_tmax, on='Estacion', how='left')\n",
    "                merged_tmin = pd.merge(Pronostico_tmin, tabla_promedio_tmin, on='Estacion', how='left')\n",
    "\n",
    "                Pronostico = pd.DataFrame()\n",
    "                Pronostico[\"Estacion\"] = merged_tmax[\"Estacion\"]\n",
    "\n",
    "                Pronostico[\"Tmax\"] = merged_tmax[\"Tmax\"] - merged_tmax[f\"{text}\"].fillna(0)\n",
    "                Pronostico[\"Tmin\"] = merged_tmin[\"Tmin\"] - merged_tmin[f\"{text}\"].fillna(0)\n",
    "\n",
    "                # Guarda el DataFrame del ajuste del pronóstico de mañana en archivos CSV y TXT\n",
    "                Pronostico.to_csv(os.path.join(carpeta_modelo, 'WRF_Ajustado', f\"{text}.temperatura.csv\"), index=False)\n",
    "                Pronostico.to_csv(os.path.join(carpeta_modelo, 'WRF_Ajustado', f\"{text}.temperatura.txt\"), index=False)\n",
    "        else:\n",
    "            print(\"Alguna de las tablas (tabla_promedio_tmax, tabla_promedio_tmin y file) o ambas están vacías\")\n",
    "    else:\n",
    "        print(\"Alguna de las tablas (tabla_promedio_tmax, tabla_promedio_tmin y file) no está definida\")\n",
    "else:\n",
    "    print(\"Alguna de las tablas (tabla_promedio_tmax, tabla_promedio_tmin y file) no existe en el entorno actual.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b5f731",
   "metadata": {},
   "source": [
    "**Hay estaciones que aveces no tienen un datos, puedo no ajustar el pronostico si el valor es NaN?\n",
    "Ejemplo: Estacion 69699 ahorita tiene el sensor de Temp en mantenimiento, si hago el ajuste el pronostico ajustado será NaN para esas estaciones, cuando use ese pronostico para ajustar otro día seguirá dando NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e2e75196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estacion</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>Tmin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69633</td>\n",
       "      <td>28.418466</td>\n",
       "      <td>20.716702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69647</td>\n",
       "      <td>26.445390</td>\n",
       "      <td>21.639424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69679</td>\n",
       "      <td>29.729584</td>\n",
       "      <td>22.144937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69699</td>\n",
       "      <td>26.070000</td>\n",
       "      <td>22.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69701</td>\n",
       "      <td>29.746071</td>\n",
       "      <td>21.201305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69709</td>\n",
       "      <td>29.399662</td>\n",
       "      <td>21.199941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>69711</td>\n",
       "      <td>30.062821</td>\n",
       "      <td>24.147281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>69713</td>\n",
       "      <td>30.389170</td>\n",
       "      <td>20.085436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>69715</td>\n",
       "      <td>29.509849</td>\n",
       "      <td>20.935085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>69717</td>\n",
       "      <td>29.161191</td>\n",
       "      <td>21.083081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>69721</td>\n",
       "      <td>27.242218</td>\n",
       "      <td>21.012224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>69723</td>\n",
       "      <td>30.050777</td>\n",
       "      <td>21.968559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>69729</td>\n",
       "      <td>29.266944</td>\n",
       "      <td>21.346111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>69731</td>\n",
       "      <td>26.480000</td>\n",
       "      <td>20.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>69735</td>\n",
       "      <td>28.267690</td>\n",
       "      <td>21.049640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>69737</td>\n",
       "      <td>25.126197</td>\n",
       "      <td>16.334985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>69743</td>\n",
       "      <td>23.412560</td>\n",
       "      <td>18.440476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>69749</td>\n",
       "      <td>25.093353</td>\n",
       "      <td>21.362303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>71015</td>\n",
       "      <td>29.894966</td>\n",
       "      <td>20.171470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>72159</td>\n",
       "      <td>32.915665</td>\n",
       "      <td>21.082258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>72163</td>\n",
       "      <td>31.149607</td>\n",
       "      <td>22.670361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>72167</td>\n",
       "      <td>34.494604</td>\n",
       "      <td>21.812420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>72181</td>\n",
       "      <td>32.072099</td>\n",
       "      <td>23.573426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>72183</td>\n",
       "      <td>30.600706</td>\n",
       "      <td>20.147343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>72189</td>\n",
       "      <td>31.890000</td>\n",
       "      <td>22.530000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>72191</td>\n",
       "      <td>30.154446</td>\n",
       "      <td>21.861774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>72195</td>\n",
       "      <td>28.080352</td>\n",
       "      <td>6.122981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>73123</td>\n",
       "      <td>20.596489</td>\n",
       "      <td>14.525379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>73137</td>\n",
       "      <td>15.805775</td>\n",
       "      <td>5.236176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>73141</td>\n",
       "      <td>18.977920</td>\n",
       "      <td>14.774564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>73145</td>\n",
       "      <td>30.451103</td>\n",
       "      <td>20.742576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>73147</td>\n",
       "      <td>30.542829</td>\n",
       "      <td>20.814605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>73149</td>\n",
       "      <td>22.082715</td>\n",
       "      <td>16.695380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>73151</td>\n",
       "      <td>25.792054</td>\n",
       "      <td>18.979242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>73153</td>\n",
       "      <td>20.230000</td>\n",
       "      <td>13.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>73155</td>\n",
       "      <td>24.544069</td>\n",
       "      <td>18.717566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>73159</td>\n",
       "      <td>33.908220</td>\n",
       "      <td>21.257089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>73167</td>\n",
       "      <td>22.879195</td>\n",
       "      <td>16.598920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>73169</td>\n",
       "      <td>29.153601</td>\n",
       "      <td>19.813737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>73171</td>\n",
       "      <td>20.749600</td>\n",
       "      <td>15.675452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>74051</td>\n",
       "      <td>33.355574</td>\n",
       "      <td>21.888697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>74053</td>\n",
       "      <td>32.117896</td>\n",
       "      <td>18.606857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>74063</td>\n",
       "      <td>32.573052</td>\n",
       "      <td>25.376569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>74071</td>\n",
       "      <td>25.737892</td>\n",
       "      <td>20.559735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>74081</td>\n",
       "      <td>32.056379</td>\n",
       "      <td>21.419561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>76059</td>\n",
       "      <td>32.967812</td>\n",
       "      <td>25.945098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>76061</td>\n",
       "      <td>27.501148</td>\n",
       "      <td>21.027938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>76063</td>\n",
       "      <td>28.440000</td>\n",
       "      <td>22.060000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>78033</td>\n",
       "      <td>31.514208</td>\n",
       "      <td>22.492934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>78035</td>\n",
       "      <td>33.840197</td>\n",
       "      <td>24.278701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>81005</td>\n",
       "      <td>30.191173</td>\n",
       "      <td>28.130539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>82011</td>\n",
       "      <td>31.887961</td>\n",
       "      <td>22.543648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>82013</td>\n",
       "      <td>31.850000</td>\n",
       "      <td>22.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>82015</td>\n",
       "      <td>33.053900</td>\n",
       "      <td>21.502172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>82019</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>22.520000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>84139</td>\n",
       "      <td>22.580000</td>\n",
       "      <td>16.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>84141</td>\n",
       "      <td>22.166893</td>\n",
       "      <td>17.329559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>84181</td>\n",
       "      <td>19.260392</td>\n",
       "      <td>14.183211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>84185</td>\n",
       "      <td>28.177760</td>\n",
       "      <td>18.761867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>84187</td>\n",
       "      <td>26.723231</td>\n",
       "      <td>18.187662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>84189</td>\n",
       "      <td>21.375547</td>\n",
       "      <td>14.824036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>84197</td>\n",
       "      <td>26.900925</td>\n",
       "      <td>18.656526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>84199</td>\n",
       "      <td>26.175313</td>\n",
       "      <td>19.173569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>84207</td>\n",
       "      <td>19.506129</td>\n",
       "      <td>11.680987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>84209</td>\n",
       "      <td>26.010319</td>\n",
       "      <td>21.007433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>84213</td>\n",
       "      <td>18.691199</td>\n",
       "      <td>10.848475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>84215</td>\n",
       "      <td>18.599647</td>\n",
       "      <td>11.908555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>84217</td>\n",
       "      <td>21.150000</td>\n",
       "      <td>13.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>84219</td>\n",
       "      <td>20.252480</td>\n",
       "      <td>15.944317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>84243</td>\n",
       "      <td>23.223068</td>\n",
       "      <td>18.422899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>84249</td>\n",
       "      <td>20.781451</td>\n",
       "      <td>14.804303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>84251</td>\n",
       "      <td>20.086602</td>\n",
       "      <td>12.215926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>84285</td>\n",
       "      <td>26.240000</td>\n",
       "      <td>18.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>84295</td>\n",
       "      <td>28.640000</td>\n",
       "      <td>16.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>85025</td>\n",
       "      <td>29.880000</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>86013</td>\n",
       "      <td>29.366123</td>\n",
       "      <td>22.189288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>87013</td>\n",
       "      <td>29.700000</td>\n",
       "      <td>22.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>88047</td>\n",
       "      <td>17.610841</td>\n",
       "      <td>13.408518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>88049</td>\n",
       "      <td>30.609170</td>\n",
       "      <td>22.807956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>88051</td>\n",
       "      <td>23.447412</td>\n",
       "      <td>16.839445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>88055</td>\n",
       "      <td>22.902657</td>\n",
       "      <td>16.352313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>90015</td>\n",
       "      <td>30.605835</td>\n",
       "      <td>22.802943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>94013</td>\n",
       "      <td>10.031892</td>\n",
       "      <td>2.248805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>94015</td>\n",
       "      <td>24.068720</td>\n",
       "      <td>18.718761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>96003</td>\n",
       "      <td>29.758493</td>\n",
       "      <td>24.515134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>98075</td>\n",
       "      <td>26.837520</td>\n",
       "      <td>17.892788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>98087</td>\n",
       "      <td>29.607713</td>\n",
       "      <td>21.986372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>98091</td>\n",
       "      <td>13.462437</td>\n",
       "      <td>3.454876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>98097</td>\n",
       "      <td>25.619415</td>\n",
       "      <td>19.076398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>98107</td>\n",
       "      <td>27.078232</td>\n",
       "      <td>17.906895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>98109</td>\n",
       "      <td>25.509914</td>\n",
       "      <td>17.883127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>100641</td>\n",
       "      <td>31.738203</td>\n",
       "      <td>22.402503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>100649</td>\n",
       "      <td>30.981891</td>\n",
       "      <td>23.195979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>100651</td>\n",
       "      <td>30.358886</td>\n",
       "      <td>20.751674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>100653</td>\n",
       "      <td>30.212914</td>\n",
       "      <td>21.583937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100655</td>\n",
       "      <td>30.559269</td>\n",
       "      <td>22.174749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Estacion       Tmax       Tmin\n",
       "0      69633  28.418466  20.716702\n",
       "1      69647  26.445390  21.639424\n",
       "2      69679  29.729584  22.144937\n",
       "3      69699  26.070000  22.050000\n",
       "4      69701  29.746071  21.201305\n",
       "5      69709  29.399662  21.199941\n",
       "6      69711  30.062821  24.147281\n",
       "7      69713  30.389170  20.085436\n",
       "8      69715  29.509849  20.935085\n",
       "9      69717  29.161191  21.083081\n",
       "10     69721  27.242218  21.012224\n",
       "11     69723  30.050777  21.968559\n",
       "12     69729  29.266944  21.346111\n",
       "13     69731  26.480000  20.120000\n",
       "14     69735  28.267690  21.049640\n",
       "15     69737  25.126197  16.334985\n",
       "16     69743  23.412560  18.440476\n",
       "17     69749  25.093353  21.362303\n",
       "18     71015  29.894966  20.171470\n",
       "19     72159  32.915665  21.082258\n",
       "20     72163  31.149607  22.670361\n",
       "21     72167  34.494604  21.812420\n",
       "22     72181  32.072099  23.573426\n",
       "23     72183  30.600706  20.147343\n",
       "24     72189  31.890000  22.530000\n",
       "25     72191  30.154446  21.861774\n",
       "26     72195  28.080352   6.122981\n",
       "27     73123  20.596489  14.525379\n",
       "28     73137  15.805775   5.236176\n",
       "29     73141  18.977920  14.774564\n",
       "30     73145  30.451103  20.742576\n",
       "31     73147  30.542829  20.814605\n",
       "32     73149  22.082715  16.695380\n",
       "33     73151  25.792054  18.979242\n",
       "34     73153  20.230000  13.390000\n",
       "35     73155  24.544069  18.717566\n",
       "36     73159  33.908220  21.257089\n",
       "37     73167  22.879195  16.598920\n",
       "38     73169  29.153601  19.813737\n",
       "39     73171  20.749600  15.675452\n",
       "40     74051  33.355574  21.888697\n",
       "41     74053  32.117896  18.606857\n",
       "42     74063  32.573052  25.376569\n",
       "43     74071  25.737892  20.559735\n",
       "44     74081  32.056379  21.419561\n",
       "45     76059  32.967812  25.945098\n",
       "46     76061  27.501148  21.027938\n",
       "47     76063  28.440000  22.060000\n",
       "48     78033  31.514208  22.492934\n",
       "49     78035  33.840197  24.278701\n",
       "50     81005  30.191173  28.130539\n",
       "51     82011  31.887961  22.543648\n",
       "52     82013  31.850000  22.370000\n",
       "53     82015  33.053900  21.502172\n",
       "54     82019  31.000000  22.520000\n",
       "55     84139  22.580000  16.330000\n",
       "56     84141  22.166893  17.329559\n",
       "57     84181  19.260392  14.183211\n",
       "58     84185  28.177760  18.761867\n",
       "59     84187  26.723231  18.187662\n",
       "60     84189  21.375547  14.824036\n",
       "61     84197  26.900925  18.656526\n",
       "62     84199  26.175313  19.173569\n",
       "63     84207  19.506129  11.680987\n",
       "64     84209  26.010319  21.007433\n",
       "65     84213  18.691199  10.848475\n",
       "66     84215  18.599647  11.908555\n",
       "67     84217  21.150000  13.160000\n",
       "68     84219  20.252480  15.944317\n",
       "69     84243  23.223068  18.422899\n",
       "70     84249  20.781451  14.804303\n",
       "71     84251  20.086602  12.215926\n",
       "72     84285  26.240000  18.660000\n",
       "73     84295  28.640000  16.810000\n",
       "74     85025  29.880000  23.000000\n",
       "75     86013  29.366123  22.189288\n",
       "76     87013  29.700000  22.670000\n",
       "77     88047  17.610841  13.408518\n",
       "78     88049  30.609170  22.807956\n",
       "79     88051  23.447412  16.839445\n",
       "80     88055  22.902657  16.352313\n",
       "81     90015  30.605835  22.802943\n",
       "82     94013  10.031892   2.248805\n",
       "83     94015  24.068720  18.718761\n",
       "84     96003  29.758493  24.515134\n",
       "85     98075  26.837520  17.892788\n",
       "86     98087  29.607713  21.986372\n",
       "87     98091  13.462437   3.454876\n",
       "88     98097  25.619415  19.076398\n",
       "89     98107  27.078232  17.906895\n",
       "90     98109  25.509914  17.883127\n",
       "91    100641  31.738203  22.402503\n",
       "92    100649  30.981891  23.195979\n",
       "93    100651  30.358886  20.751674\n",
       "94    100653  30.212914  21.583937\n",
       "95    100655  30.559269  22.174749"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pronostico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "b9b555ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar opciones para mostrar todas las filas y columnas sin truncar\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Restaurar las opciones predeterminadas si es necesario\n",
    "#pd.reset_option('display.max_rows')\n",
    "#pd.reset_option('display.max_columns')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
